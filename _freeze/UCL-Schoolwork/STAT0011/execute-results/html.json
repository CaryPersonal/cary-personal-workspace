{
  "hash": "0df9d4e1d7ebf33c2929c0f155832aaa",
  "result": {
    "markdown": "---\ntitle: STAT0011\nsubtitle: Decision and Risk\nexecute: \n  eval: false\n---\n\n\n# [**Z Score Calculator**](https://www.calculator.net/z-score-calculator.html)\n\n# [**Normal Distribution Calculator**](https://www.hackmath.net/en/calculator/normal-distribution)\n\n# **Important Distributions**\n\n## **Continuous Distributions**\n\n### **Normal Distribution (Gaussian Distribution)**\n\n![](images/image-2146374466.png)\n\n### **Uniform Distribution**\n\n![](images/image-1370233818.png)\n\n### **Gamma Distribution**\n\n![](images/image-267642431.png)\n\n# **Copula Analysis Using R**\n\n## **Week 5 Data**\n\nRequired Packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsuppressPackageStartupMessages({\n  library(VineCopula) # Copula analysis\n  library(ADGofTest) # Anderson-Darling Goodness-of-Fit test\n  library(KScorrect) # (Lilliefors-Corrected) Kolmogorov-Smirnov Goodness-of-Fit test\n  library(fGarch) # Time series analysis\n})\n```\n:::\n\n\nLoad the data and have a look:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nload(\"week5data.RData\")\nplot(dataset)\n```\n\n::: {.cell-output-display}\n![](STAT0011_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nThis does not look like a sample from a bivariate normal distribution! In fact, `ret1` follows a Student-t distribution with 10 degrees of freedom (df); `ret2` follows a Student-t distribution with 6 df. Let's assume we have this information.\n\nNext we apply **Probability Integral Transform (PIT)** to `ret1` which has Student-t distribution with 10 df:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nu1 <- pstd(dataset$ret1, mean = 0, sd = 1, nu = 10)\nhist(u1)\n```\n\n::: {.cell-output-display}\n![](STAT0011_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nImplement Kolmogorov-Smirnov test:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nKStest1 <- LcKS(u1, cdf = \"punif\")\nKStest1$p.value\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.3802\n```\n:::\n:::\n\n\nImplement Anderson-Darling test:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nADtest1 <- ad.test(u1, null = \"punif\")\nADtest1$p.value\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       AD \n0.3243088 \n```\n:::\n:::\n\n\nThen we apply **Probability Integral Transform (PIT)** to `ret2` which has Student-t distribution with 6 df:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nu2 <- pstd(dataset$ret2, mean = 0, sd = 1, nu = 6)\nhist(u2)\n```\n\n::: {.cell-output-display}\n![](STAT0011_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nImplement Kolmogorov-Smirnov test:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nKStest2 <- LcKS(u2, cdf = \"punif\")\nKStest2$p.value\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.2172\n```\n:::\n:::\n\n\nImplement Anderson-Darling test:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nADtest2 <- ad.test(u1, null = \"punif\")\nADtest2$p.value\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       AD \n0.3243088 \n```\n:::\n:::\n\n\nWe **pass** the **test for uniformity** for both transformed log-returns, so we can proceed to copula modelling.\n\nUsing `BiCopSelect` function fit various copulas to the dataset and select the copula that provides the best fit based on the **AIC** criterion:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(cbind(u1, u2))\n```\n\n::: {.cell-output-display}\n![](STAT0011_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code}\nmodel <- BiCopSelect(u1, u2, familyset = NA, selectioncrit = \"AIC\", indeptest = TRUE, level = 0.05)\nmodel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBivariate copula: Clayton (par = 1.94, tau = 0.49) \n```\n:::\n:::\n\n\nThe model of best fit is **Clayton with the estimated parameter theta = 1.94**.\n\nNext we estimate the Value-at-Risk using the **Monte Carlo simulation** approach based on copula theory:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nN <- 1800\nset.seed(0123)\nu_sim <- BiCopSim(N, family = 3, model$par) # family = 3 means Clayton\n```\n:::\n\n\nNext we apply the component-wise **Inverse Probability Integral Transform (IPIT)** to both `ret1` and `ret2`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nret1_sim <- qstd(u_sim[,1], mean = 0, sd = 1, nu = 10)\nret2_sim <- qstd(u_sim[,2], mean = 0, sd = 1, nu = 6)\n```\n:::\n\n\nNote that our selected copula model is capable of generating values of log-returns with the observed dependence structure:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(2,1))\nplot(dataset, ylab = \"ret2\", xlab = \"ret1\", col = \"blue\", main = \"Original log-returns\")\nplot(data.frame(ret1_sim, ret2_sim), ylab = \"ret2_sim\", xlab = \"ret1_sim\",\n     col = \"blue\", main = \"Simulated log-returns\")\n```\n\n::: {.cell-output-display}\n![](STAT0011_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mfrow=c(1,1))\n```\n:::\n\n\nNext, compute portfolio log-returns:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nport_sim <- matrix(0, nrow = N, ncol = 1)\nVaR_sim <- matrix(0, nrow = 1, ncol = 2)\n\nport_sim <- log(1 + ((exp(ret1_sim) - 1) + (exp(ret2_sim) - 1)) * (1 / 2)) # Need to transform individual log-returns back to individual net returns, and compute portfolio net returns, finally compute portfolio log-returns.\n```\n:::\n\n\nReturn the estimated 99% and 95% Value-at-Risk estimates:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvar_sim <- quantile(port_sim, c(0.01, 0.05))\nvar_sim\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       1%        5% \n-2.244965 -1.512537 \n```\n:::\n:::\n\n\n# **Time Series Analysis Using R**\n\n## **Week 6 Data**\n\nRequired Packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsuppressPackageStartupMessages({\n  library(VineCopula) # Copula analysis\n  library(ADGofTest) # Anderson-Darling Goodness-of-Fit test\n  library(KScorrect) # (Lilliefors-Corrected) Kolmogorov-Smirnov Goodness-of-Fit test\n  library(fGarch) # Time series analysis\n  library(tseries)\n})\n```\n:::\n\n\nLoad the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nload(\"week6data.RData\")\nret1 <- dataset[,1]\nret2 <- dataset[,2]\n```\n:::\n\n\nUse the **Jarque--Bera test**, a [goodness-of-fit](https://en.wikipedia.org/wiki/Goodness-of-fit \"Goodness-of-fit\") test of whether sample data have the [skewness](https://en.wikipedia.org/wiki/Skewness \"Skewness\") and [kurtosis](https://en.wikipedia.org/wiki/Kurtosis \"Kurtosis\") matching a [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution \"Normal distribution\"):\n\n\n::: {.cell}\n\n```{.r .cell-code}\njarque.bera.test(ret1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tJarque Bera Test\n\ndata:  ret1\nX-squared = 54.62, df = 2, p-value = 1.379e-12\n```\n:::\n\n```{.r .cell-code}\njarque.bera.test(ret2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tJarque Bera Test\n\ndata:  ret2\nX-squared = 888.08, df = 2, p-value < 2.2e-16\n```\n:::\n:::\n\n\n### **Model for the Conditional Mean: AR Models - the Box-Jenkins Approach**\n\n#### **Step 1: Identification**\n\n**Returns 1**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(ret1, type = \"l\")\n```\n\n::: {.cell-output-display}\n![](STAT0011_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mfrow = c(2, 2))\nacf(ret1, col = \"green\", lwd = 2)\npacf(ret1, col = \"green\", lwd = 2)\nacf(ret1^2, col = \"red\", lwd = 2) # for GARCH effects\npar(mfrow = c(1, 1))\n```\n\n::: {.cell-output-display}\n![](STAT0011_files/figure-html/unnamed-chunk-18-2.png){width=672}\n:::\n:::\n\n\nWe can determine the lags of AR model via the **Partial ACF plot**.\n\nWe can use the ACF plot for the squared returns to see if we need to introduce **GARCH effects**, we need to try `garch(1, 1)` up until `garch(3, 3)`.\n\n**Returns 2**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(ret2, type = \"l\")\n```\n\n::: {.cell-output-display}\n![](STAT0011_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mfrow = c(2, 2))\nacf(ret2, col = \"green\", lwd = 2)\npacf(ret2, col = \"green\", lwd = 2)\nacf(ret2^2, col = \"red\", lwd = 2) # for GARCH effects\npar(mfrow = c(1, 1))\n```\n\n::: {.cell-output-display}\n![](STAT0011_files/figure-html/unnamed-chunk-19-2.png){width=672}\n:::\n:::\n\n\nWe can determine the lags of AR model via the **Partial ACF plot**.\n\nWe can use the ACF plot for the squared returns to see if we need to introduce **GARCH effects**, we need to try `garch(1, 1)` up until `garch(3, 3)`.\n\n#### **Step 2: Estimation**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel1 <- garchFit(formula = ~ arma(1, 0) + garch(1,1), data = ret1, trace = F, cond.dist = \"norm\")\nmodel2 <- garchFit(formula = ~ arma(3, 0) + garch(1,1), data = ret2, trace = F, cond.dist = \"norm\")\n```\n:::\n\n\nNote: this function uses **all first three lags**, not lag 3 only.\n\nWe can plot the histogram of `ret1` and `ret2` to decide the right distribution for argument `cond.dist =` . We also need to use Kolmogorov-Smirnov test and Anderson-Darling test (see later).\n\n#### **Step 3: Model Checking**\n\n**Returns 1**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nres1 <- residuals(model1, standardize = TRUE)\n\npar(mfrow = c(2, 1))\nacf(res1, col = \"green\", lwd = 2)\nacf(res1^2, col = \"red\", lwd = 2)\n```\n\n::: {.cell-output-display}\n![](STAT0011_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mfrow = c(1, 1))\n\n# The Box-Ljung test is to test if there exists autocorrelation, the null is no autocorrelation. We use fitdf = 1 because we used AR(1) model earlier.\nBox.test(res1, lag = 10, type = c(\"Ljung-Box\"), fitdf = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tBox-Ljung test\n\ndata:  res1\nX-squared = 8.495, df = 9, p-value = 0.4851\n```\n:::\n\n```{.r .cell-code}\nBox.test(res1^2, lag = 10, type = c(\"Ljung-Box\"), fitdf = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tBox-Ljung test\n\ndata:  res1^2\nX-squared = 11.657, df = 9, p-value = 0.2333\n```\n:::\n\n```{.r .cell-code}\nmodel1@fit$ics\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      AIC       BIC       SIC      HQIC \n0.5658129 0.5798152 0.5658005 0.5709543 \n```\n:::\n\n```{.r .cell-code}\nu1 <- pnorm(res1, mean = 0, sd = 1)[4:length(ret1)]\nhist(u1)\n```\n\n::: {.cell-output-display}\n![](STAT0011_files/figure-html/unnamed-chunk-21-2.png){width=672}\n:::\n:::\n\n\n**Further distributional checks:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Kolmogorov-Smirnov test\nKStest1 <- LcKS(u1, cdf = \"punif\")\nKStest1$p.value\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.418\n```\n:::\n\n```{.r .cell-code}\n#Anderson-Darling test\nADtest1 <- ad.test(u1, null = \"punif\")\nADtest1$p.value\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       AD \n0.4202428 \n```\n:::\n:::\n\n\n**Returns 2**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nres2 <- residuals(model2, standardize = TRUE)\n\npar(mfrow = c(2, 1))\nacf(res2, col = \"green\", lwd = 2)\nacf(res2^2, col = \"red\", lwd = 2)\n```\n\n::: {.cell-output-display}\n![](STAT0011_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mfrow = c(1, 1))\n\n# The Box-Ljung test is to test if there exists autocorrelation, the null is no autocorrelation. We use fitdf = 3 because we used AR(3) model earlier.\nBox.test(res2, lag = 10, type = c(\"Ljung-Box\"), fitdf = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tBox-Ljung test\n\ndata:  res2\nX-squared = 4.4836, df = 7, p-value = 0.7227\n```\n:::\n\n```{.r .cell-code}\nBox.test(res2^2, lag = 10, type = c(\"Ljung-Box\"), fitdf = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tBox-Ljung test\n\ndata:  res2^2\nX-squared = 7.5536, df = 7, p-value = 0.3736\n```\n:::\n\n```{.r .cell-code}\nmodel2@fit$ics\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     AIC      BIC      SIC     HQIC \n2.011268 2.030871 2.011244 2.018466 \n```\n:::\n\n```{.r .cell-code}\nu2 <- pnorm(res2, mean = 0, sd = 1)[4:length(ret2)]\nhist(u2)\n```\n\n::: {.cell-output-display}\n![](STAT0011_files/figure-html/unnamed-chunk-23-2.png){width=672}\n:::\n:::\n\n\n**Further distributional checks:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Kolmogorov-Smirnov test\nKStest2 <- LcKS(u2, cdf = \"punif\")\nKStest2$p.value\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.6628\n```\n:::\n\n```{.r .cell-code}\n#Anderson-Darling test\nADtest2 <- ad.test(u2, null = \"punif\")\nADtest2$p.value\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      AD \n0.880416 \n```\n:::\n:::\n\n\n##### Misspecification Example 1: Using AIC and BIC\n\nLet's deliberately choosing the \"wrong\" AR(1) model instead of AR(3) model to show that the AIC and BIC are indeed helpful model selection criteria!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel2b <- garchFit(formula = ~arma(1,0) + garch(1,1), data = ret2, trace = F, cond.dist = \"norm\")\nres2b <- residuals(model2b, standardize = TRUE)\n\npar(mfrow = c(1, 2))\nacf(res2b, col = \"green\", lwd = 2)\nacf(res2b^2, col = \"red\", lwd = 2)\n```\n\n::: {.cell-output-display}\n![](STAT0011_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mfrow = c(1, 1))\n\nBox.test(res2b, lag = 10, type = c(\"Ljung-Box\"), fitdf = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tBox-Ljung test\n\ndata:  res2b\nX-squared = 784.15, df = 9, p-value < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\nBox.test(res2b^2, lag = 10, type = c(\"Ljung-Box\"), fitdf = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tBox-Ljung test\n\ndata:  res2b^2\nX-squared = 90.379, df = 9, p-value = 1.332e-15\n```\n:::\n\n```{.r .cell-code}\nu2b <- pnorm(res2b, mean = 0, sd = 1)\nhist(u2b)\n```\n\n::: {.cell-output-display}\n![](STAT0011_files/figure-html/unnamed-chunk-25-2.png){width=672}\n:::\n\n```{.r .cell-code}\n#Kolmogorov-Smirnov test\nKStest2b <- LcKS(u2b, cdf = \"punif\")\nKStest2b$p.value\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.3402\n```\n:::\n\n```{.r .cell-code}\n#Anderson-Darling test\nADtest2b <- ad.test(u2b, null = \"punif\")\nADtest2b$p.value\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       AD \n0.4864833 \n```\n:::\n:::\n\n\n##### Misspecification Example 2: Using Student-t instead of Normal\n\nLet's deliberately choose the \"wrong\" Student-t distribution instead of a Normal distribution. We know that as [the df parameter approaches infinity, the Student-t distribution approaches the Normal distribution]{.underline}. So for the sufficiently large value of (estimated) df parameter, we would not expect to see a big difference between the selection criteria.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel2c <- garchFit(formula = ~arma(3,0) + garch(1,1), data = ret2, trace = F, cond.dist = \"std\")\nres2c <- residuals(model2c, standardize = TRUE)\n\npar(mfrow = c(1, 2))\nacf(res2c, col = \"green\", lwd = 2)\nacf(res2c^2, col = \"red\", lwd = 2)\n```\n\n::: {.cell-output-display}\n![](STAT0011_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mfrow = c(1, 1))\n\nBox.test(res2c, lag = 10, type = c(\"Ljung-Box\"), fitdf = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tBox-Ljung test\n\ndata:  res2c\nX-squared = 4.7051, df = 9, p-value = 0.8592\n```\n:::\n\n```{.r .cell-code}\nBox.test(res2c^2, lag = 10, type = c(\"Ljung-Box\"), fitdf = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tBox-Ljung test\n\ndata:  res2c^2\nX-squared = 7.8925, df = 9, p-value = 0.545\n```\n:::\n\n```{.r .cell-code}\ncoef(model2c)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          mu          ar1          ar2          ar3        omega       alpha1 \n 0.030941630  0.026238762 -0.004798470  0.561470633  0.009426896  0.094068483 \n       beta1        shape \n 0.893152245 10.000000000 \n```\n:::\n\n```{.r .cell-code}\nu2c <- pnorm(res2c, mean = 0, sd = 1)\nhist(u2c)\n```\n\n::: {.cell-output-display}\n![](STAT0011_files/figure-html/unnamed-chunk-26-2.png){width=672}\n:::\n\n```{.r .cell-code}\n#Kolmogorov-Smirnov test\nKStest2c <- LcKS(u2c, cdf = \"punif\")\nKStest2c$p.value\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.533\n```\n:::\n\n```{.r .cell-code}\n#Anderson-Darling test\nADtest2c <- ad.test(u2c, null = \"punif\")\nADtest2c$p.value\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       AD \n0.5259121 \n```\n:::\n:::\n\n\n#### **Compare Three Models**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nic <- rbind(model2@fit$ics, model2b@fit$ics, model2c@fit$ics)\nrownames(ic) <- c(\"AR(3)\", \"AR(1)\", \"Student-t\")\nic\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n               AIC      BIC      SIC     HQIC\nAR(3)     2.011268 2.030871 2.011244 2.018466\nAR(1)     2.371386 2.385388 2.371373 2.376527\nStudent-t 2.019908 2.042311 2.019876 2.028134\n```\n:::\n:::\n\n\nLet's compare p-values for the KS and AD tests for uniformity:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndtests <- rbind(c(KStest2$p.value, ADtest2$p.value), c(KStest2b$p.value, ADtest2b$p.value), c(KStest2c$p.value, ADtest2c$p.value))\nrownames(dtests) <- c(\"AR(3)\", \"AR(1)\", \"Student-t\")\ndtests\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                        AD\nAR(3)     0.6628 0.8804160\nAR(1)     0.3402 0.4864833\nStudent-t 0.5330 0.5259121\n```\n:::\n:::\n\n\n#### **Copula Modelling**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- BiCopSelect(u1, u2, familyset = NA, selectioncrit = \"AIC\", indeptest = TRUE, level = 0.05, se = TRUE)\nmodel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBivariate copula: Clayton (par = 3.01, tau = 0.6) \n```\n:::\n:::\n\n\nValue-at-Risk uisng Monte Carlo simulation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nN <- 10000\nu_sim <- BiCopSim(N, family = model$family, model$par,  model$par2)\n```\n:::\n\n\nHere we are assuming marginal models are N(0,1), completely ignoring our knowledge of real DGP\n\n\n::: {.cell}\n\n```{.r .cell-code}\nres1_sim <- qnorm(u_sim[,1], mean = 0, sd = 1) \nres2_sim <- qnorm(u_sim[,2], mean = 0, sd = 1) \n```\n:::\n\n\nHowever, \"res1_sim\" and \"res2_sim\" are i.i.d. So, the next step is to **re-introduce autocorrelation and GARCH effects** observed in data.\n\nThis section has been omitted as this will be part of your ICA group assignment...\n\n99% and 95% VaR:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nportsim <- matrix(0, nrow = N, ncol = 1)\nvarsim <- matrix(0, nrow = 1, ncol = 2)\n\nportsim <- log(1+((exp(y1simulated) - 1) + (exp(y2simulated) - 1)) * (1/2))\nvarsim <- quantile(portsim, c(0.01, 0.05))\nvarsim\n```\n:::\n",
    "supporting": [
      "STAT0011_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}