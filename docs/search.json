[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cary’s Personal Workspace",
    "section": "",
    "text": "Important DDLs\n\n\n\n\n\n\n\n\nSubject\nDue Date\nStatus\n\n\n\n\nSTAT0024 Coursework 3\n16:00, Tuesday, 14 March 2023\n\n\n\nSTAT0024 ICA\n16:00, Tuesday, 7 March 2023\nPending Review\n\n\nECON0047 MCQ\n9:00-10:00, Wednesday, 22 March 2023\n\n\n\nSTAT0011 ICA\n16:00, Friday, 24 March 2023\nIn Progress\n\n\n\n\n\nIndividual Examination Timetable\n\n\n\nExamination\nDate\nTime\nDuration\n\n\n\n\nSTAT0025\nWednesday, 26 April 2023\n14:30\n2:40\n\n\nSTAT0011\nThursday, 27 April 2023\n10:00\n2:40\n\n\nMATH0050\nFriday, 5 May 2023\n10:00\n2:10\n\n\nECON0022\nTuesday, 16 May 2023\n10:00\n2:20\n\n\nSTAT0024\nThursday, 18 May 2023\n14:30\n2:30"
  },
  {
    "objectID": "index.html#what-is-quarto",
    "href": "index.html#what-is-quarto",
    "title": "Cary’s Personal Workspace",
    "section": "What is Quarto?",
    "text": "What is Quarto?\nQuarto helps you have your ideas and your code in one place, and present it in a beautiful way.\nQuarto unifies and extends the RMarkdown ecosystem - it unifies by combining the functionality of R Markdown, bookdown, distill, xaringian, etc into a single consistent system. And it extends in several ways: all features are possible beyond R too, including Python and Javascript. It also has more “guardrails”: accessibility and inclusion are centered in the design. Quarto is for people who love RMarkdown, and it’s for people who have never used RMarkdown.\nThe ability for Quarto to streamline collaboration has been so cool and important for our NASA Openscapes project. Quarto has been a common place for us to collaborate - across R and Python languages and coding expertise."
  },
  {
    "objectID": "index.html#what-is-this-tutorial",
    "href": "index.html#what-is-this-tutorial",
    "title": "Cary’s Personal Workspace",
    "section": "What is this tutorial?",
    "text": "What is this tutorial?\nThis is a 1-hour tutorial that can be used to teach or as self-paced learning.\nWe introduce Quarto by exploring this tutorial website, and practicing the basic Quarto workflow using different tools (GitHub browser, RStudio, and Jupyter) for editing your website.\nWe’ll start off from the browser so you don’t need to install any additional software, however this approach is very limited and you will soon outgrow its capabilities. If you don’t already have a workflow to edit files and sync to GitHub from your computer, I recommend RStudio. You don’t need to know R to use RStudio, and it has powerful editor features that make for happy workflows.\nQuarto.org is the go-to place for full documentation and more tutorials!"
  },
  {
    "objectID": "index.html#example-quarto-sites",
    "href": "index.html#example-quarto-sites",
    "title": "Cary’s Personal Workspace",
    "section": "Example Quarto sites",
    "text": "Example Quarto sites\nA few Quarto websites from Openscapes - so far we have been using Quarto for documentation using Quarto and Markdown files and Jupyter Notebooks.\n\nChampions Lessons Series\nOpenscapes Approach Guide\n\n2021 NASA Cloud Hackathon\nFaylab Lab Manual\nA Quarto tip a day, by Mine Çetinkaya-Rundel"
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Cary’s Personal Workspace",
    "section": "About",
    "text": "About\nOpenscapes is about better science for future us. We help researchers reimagine data analysis, develop modern skills that are of immediate value to them, and cultivate collaborative and inclusive research teams as part of the broader global open movement.\nWe’re developing this tutorial to help folks with different levels of technical skills use Quarto for documentation and tutorial building. This tutorial was originally created for several different audiences: NASA-Openscapes researcher support engineers using Python, communications directors at organizations promoting open science who do not identify as coders, and fisheries scientists curious about transitioning from RMarkdown. We’re hoping it’s useful to folks with backgrounds as wide as these; if you find it useful or have suggestions for improvement, please let us know by clicking “Edit this page” or “Report an issue” at the upper right side of any page."
  },
  {
    "objectID": "explore.html",
    "href": "explore.html",
    "title": "Tools and Tips",
    "section": "",
    "text": "If iCloud gets stuck during the upload process (never finish uploading), open the terminal and try the following:\nkillall bird\nkillall cloudd"
  },
  {
    "objectID": "Tools-and-Tips.html",
    "href": "Tools-and-Tips.html",
    "title": "Tools and Tips",
    "section": "",
    "text": "TextsGraphicsVideosVoicesApps\n\n\nChatGPT: A chatbot developed by OpenAI.\nThe new Bing: The new AI-powered Bing search engine.\n\n\nDALL·E: Generate images from natural language descriptions.\nUnsplash: Beautiful free images & pictures.\nICONS8: Collections of good icons.\n\n\n\n\n\n\n\n\nDockhunt: Discover the apps everyone is docking about."
  },
  {
    "objectID": "quarto-workflows/index.html",
    "href": "quarto-workflows/index.html",
    "title": "UCL Schoolwork",
    "section": "",
    "text": "This is a test page."
  },
  {
    "objectID": "quarto-workflows/index.html#authoring",
    "href": "quarto-workflows/index.html#authoring",
    "title": "UCL Schoolwork",
    "section": "Authoring",
    "text": "Authoring\nAs an author, you have a lot of options of how your text will be formatted, arranged, and interlinked. You will be writing in Markdown, which is a lightweight text formatting language. The Quarto documentation about authoring introduces markdown-basics that will get you started. Also see Mine Çetinkaya-Rundel’s A Quarto tip a day.\nEach page of our site has a similar first few lines - this YAML, like we saw in our _quarto.yml and it is indicated by two sets of 3 dashes --- :\n---\ntitle: My title\n---\nYou’re able to add more features to individual pages by including it in the YAML, which for the most part here only includes a title. See Quarto excecution options for more information of what you can include in the YAML."
  },
  {
    "objectID": "quarto-workflows/index.html#update-_quarto.yml",
    "href": "quarto-workflows/index.html#update-_quarto.yml",
    "title": "UCL Schoolwork",
    "section": "Update _quarto.yml",
    "text": "Update _quarto.yml\nLet’s have a closer look at the _quarto.yml file.\nThis type of file (.yml or .yaml) is written in YAML (“Yet Another Markup Language”). You’ll be able to shift the arrangement of webpages by reordering/adding/deleting them in the _quarto.yml file following the patterns you see in this example.\n\n\n\n_quarto.yml and website side-by-side\n\n\nNotice that there are multiple ways in the _quarto.yml for you to include a file in your website. For example, in the above image, the “First Observations” we see in the left sidebar of the published website (right image) is represented in _quarto.yml (left image) over two lines, with line 36 indicating the file reference and line 37 indicating the text to show up in the left sidebar. However, “From RStudio” is only represented in one line of _quarto.yml, on line 43. This represents two strategies for including a file in your website. By default, the title of a specified file will show up in the website’s sidebar, which is what is happening with the “From RStudio” example. If you would like more control over what is written in the sidebar vs the title of your files, then the approach we took with “First Observations” is what you’ll want to do: you’ll see that only “First Observations” shows up in the sidebar as we specified in _quarto.yml, but the page’s title says “First Observations & Setup” (which in our preference was too long for the sidebar).\n\n\n\n\n\n\nNote\n\n\n\nAs you modify _quarto.yml, the most important thing to know is that spacing matters. Pay attention to whether text is indented by one, two, four, or other spaces, and make sure you follow it; if your site is not looking as expected it is likely a silent error in your YAML. Some text editors like RStudio provide debugging support for YAML and are highly recommended to save you time and heartache."
  },
  {
    "objectID": "quarto-workflows/index.html#install-quarto",
    "href": "quarto-workflows/index.html#install-quarto",
    "title": "UCL Schoolwork",
    "section": "Install Quarto",
    "text": "Install Quarto\nhttps://quarto.org/docs/get-started/ describes how to install Quarto, which will depend on your operating system. We’ll walk through installation for each tool in the next chapters."
  },
  {
    "objectID": "UCL-Schoolwork/index.html",
    "href": "UCL-Schoolwork/index.html",
    "title": "UCL Schoolwork",
    "section": "",
    "text": "UCL Portico\n\n\n\nQuick Links\nCitrix Workspace"
  },
  {
    "objectID": "UCL-Schoolwork/STAT0023.html",
    "href": "UCL-Schoolwork/STAT0023.html",
    "title": "STAT0023",
    "section": "",
    "text": "Required Packages\n\nsuppressPackageStartupMessages({\n  library(Hmisc)\n  library(RColorBrewer)\n  library(ggplot2)\n  library(maps)\n  library(robustbase)\n  library(mgcv) # for gam()\n})\n\n\n\n\n\n\n\n\n\n\nNA (Not Available): missing values; NaN (Not a Number): illegal operations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspecies.data <- read.table(\"galapagos.dat\", header = TRUE)\n\nWhat is the name of the (smallest) / (second largest) Galapagos island in the dataset?\n\n## the smallest\nrownames(species.data)[which.min(species.data$Area)]\n## the second largest\nrownames(species.data)[order(species.data$Area, decreasing = TRUE)[2]]\n\nHow many plant species are there in total on the second largest Galapagos island in the dataset?\n\nspecies.data$Species[order(species.data$Area, decreasing = TRUE)[2]]\n\nWhat is the name of the island with a value of 25 for the Elevation variable?\n\nrownames(species.data)[species.data$Elevation == 25]\n\nHow many islands have fewer than 25 species in total?\n\nsum(species.data$Species < 25)\n\nWhat is the estimated slope in a linear regression of Area upon Scruz?\n\nModel <- lm(Area ~ Scruz, data = species.data)\nsummary(Model)\nprint(Model$coefficients, digits = 8)\n\n\n\n\n\niris <- iris\n\nWhat is the mean of the petal widths for all of the flowers in the dataset?\n\nmean(iris$Petal.Width)\n\nWhat is the mean of the petal widths for all of the “setosa” flowers in the dataset?\n\nmean(iris$Petal.Width[iris$Species == \"setosa\"])\n## or\ntapply(iris$Petal.Width, INDEX = iris$Species, FUN = mean)\n\nConsidering the petal lengths of the flowers, which of the three species has the largest standard deviation?\n\ntapply(iris$Petal.Length, INDEX = iris$Species, FUN = sd)\n\nWhat is the 40th percentile of the petal lengths for all of the flowers in the dataset?\n\nprint(quantile(iris$Petal.Length, probs = 0.4), digits = 8)\n\nConsidering the petal lengths of the flowers, which of the three species has the largest 90th percentile?\n\ntapply(iris$Petal.Length, INDEX = iris$Species, FUN = quantile, probs = 0.9)\n\nCarry out a two-tailed t test, assuming equal variances, for a difference between the population mean petal widths for “versicolor” and “virginica” flowers.\n\nresult <- t.test(iris$Petal.Width[iris$Species == \"versicolor\"],\n                 iris$Petal.Width[iris$Species == \"virginica\"],\n                 var.equal = TRUE)\nprint(result, digits = 8)\n\nCarry out an F test of the null hypothesis that the population variance of sepal lengths is the same for flowers of “setosa” and “virginica” species.\n\nresult <- var.test(iris$Sepal.Length[iris$Species == \"setosa\"],\n                   iris$Sepal.Length[iris$Species == \"virginica\"])\nprint(result, digits = 8)\n\nTo three decimal places, what is the lower limit of a 90% confidence interval for the difference between the population mean petal widths for “versicolor” and “virginica” flowers?\n\nresult <- t.test(iris$Petal.Width[iris$Species == \"versicolor\"],\n                 iris$Petal.Width[iris$Species == \"virginica\"],\n                 var.equal = TRUE,\n                 conf.level = 0.90)\nprint(result, digits = 8)\n\n\n\n\n\n\nx <- seq(-2, 2, 0.05)\nx2 <- x^2\nx3 <- x^3\nplot(x, x2)\nlines(x, x3, lwd = 2, col = \"red\")\n\n\n\n\nChickWeight <- ChickWeight\n\n\n\n\n\nCO2 <- CO2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\niris <- iris\n\n\n\n\nModel <- lm(Petal.Length ~ Species - 1, data = iris)\nsummary(Model)\n\nThe coefficient estimates in the above model (no intercept) are just the sample means.\n\n\n\n\nModel <- lm(Petal.Length ~ Species, data = iris)\nsummary(Model)\n\nThe intercept in the above model (intercept included) is the Setosa sample mean. The coefficient estimates are the differences between the respective means and the Setosa mean.\n\n\n\n\nModel <- lm(Petal.Length ~ Species, data = iris, contrasts = list(Species = \"contr.sum\"))\nsummary(Model)\n\nSpecies 1: Setosa\nSpecies 2: Versicolor\nSpecies 3: Virginica\nThe intercept in the above model (“sum to zero” constraint) is the overall mean petal length for all the iris flowers. The coefficients represent the differences between the respective means and the overall mean.\n\n\n\n\nModel <- lm(Petal.Length ~ Species + Sepal.Length, data = iris)\nsummary(Model)\n\nThe intercept in the above model is the expected petal length for Setosa when the sepal length is zero.\n\n\n\n\nModel <- lm(Petal.Length ~ Species + Sepal.Length + Species:Sepal.Length, data = iris)\nsummary(Model)\n\nThe main effect coefficients for Versicolor and Virginica represent differences relative to Setosa. Two additional coefficients for the interactions represent differences in the slopes of the Petal.Length:Sepal.Length relationship for these two species, relative to the slope for Setosa flowers.\nNote: Interactions have nothing to do with correlations between covariates: rather, they relate to the way in which the relationship between one covariate and the response varies with another covariate.\n\n\n\n\n\nload(\"UStemps.rda\")\nustemp$longitude <- -ustemp$longitude\n\n\n\n\nModel1 <- lm(min.temp ~ latitude + longitude, data = ustemp)\nsummary(Model1)\n\n\n\n\n\nModel2 <- lm(min.temp ~ latitude + longitude + I(latitude^2) + I(longitude^2) + latitude:longitude, data = ustemp)\nsummary(Model2)\n\n\n\n\npar(mfrow = c(2,2),\n    mar = c(3,3,2,2),\n    mgp = c(2,0.75,0))\nplot(Model2, which = 1:4)\n\nCook’s Distance plot shows three observations that may be influential, they are:\n\nustemp[c(5, 13, 52), ]\n## 5: Los Angeles, CA\n## 13: Miami, FL\n## 52: Seattle, WA\n\n\n\n\n\n\nModel2Rob <- lmrob(min.temp ~ latitude + longitude + I(latitude^2) + I(longitude^2) + latitude:longitude, data = ustemp)\nsummary(Model2Rob)\n\nNote: the purpose of robust estimation is to try and include all of the observations, but to limit the extent to which any individual observation can influence the fit.\n\n\n\n\n\n\n\nThe main things that can result in incorrect standard errors are non-constant variance and dependence between residuals\n\nSystematic structure in mean residuals\n“Residuals vs Fitted Values” plot, or “Residuals vs Covariates” plot.\nSuch structure indicates that the modelled representation of the regression function is inadequate. Whether this matters is context-specific: you may decide, for example, that the residuals are all so small that your model is already predicting well enough.\nNon-constant variance\n“Residuals vs Fitted Values” plot may exhibit a “funnel” shape (i.e. the residual variance seems higher at one end than the other).\n“Scale-Location” plot: the absolute residuals tend to be larger at one end than the other.\nIn this case, the least-squares estimates are not fully efficient (i.e. you’re not making the best use of your data), and the reported standard errors will be incorrect — as will the results of any hypothesis tests and confidence interval calculations.\nNon-normal residuals\n“Normal Q-Q plot”: the residuals don’t fall roughly on a straight line.\nThis is not critical in large samples, the exception is where you want to calculate prediction intervals for future observations: these will only be accurate if the future observations do indeed have an approximate normal distribution.\nLack of independence\nCommon situations in which it might be a problem are when data are collected at successive time points, or at a collection of spatial locations. Although none of the ‘standard’ plots is designed to check for this, lack of independence can lead to apparent structure in some of the residual plots. For example, if observations are collected sequentially in time and successive residuals are highly correlated, this can give the appearance of curvature in the “Residuals vs Fitted Values” plot: the curvature is not due to a nonlinear relationship between response and covariates, but simply due to the fact that neighbouring residuals are similar to each other because they are correlated.\n\n\n\n\nRule-of-thumb: observation influential if Cook’s distance exceeds \\(\\frac{8}{n-2k}\\), where k is the number of coefficients estimated.\n\n\n\n\nTransform the response variable and / or covariate (but only if the resulting model makes scientific sense). Sometimes, a relationship can be made more linear by taking logs or square roots of one or more quantities; transforming the response variable can also help to make the residual variance more constant, and to make the assumption of normality more reasonable. Don’t take logs (or square roots) of quantities that could be negative, though!\nAdd additional terms to the model. For the temperature data for example, you might consider extending the quadratic model to include third-degree terms in latitude and longitude; or additional covariates such as altitude if these were available.\nIf the diagnostics suggest that the residual variance is related to the fitted values (e.g. the ‘residuals versus fitted values’ plot has a funnel shape) and there is good reason to suspect that the responses follow non-normal distributions (e.g. because they are counts, so that Poisson distributions might be more appropriate) then a more general class of models may be appropriate — such as generalized linear models (GLMs).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReturn TRUE if there is at least one strictly negative value in a vector x, and FALSE otherwise.\n\nany(x<0)\n\nExtract the elements of a vector x that are both (i) strictly positive (ii) divisible by 10.\n\nx[x>0&x%%10==0]\n\nExtract the rows of a matrix or data frame y for which the corresponding elements of a vector x are non-negative, where x is a vector with length equal to the number of rows of y.\n\ny[x>=0,]\n\nExtract the elements of a vector y for which the corresponding elements of x are strictly negative.\n\ny[x<0]\n\n\n\n\nCreate a function to approximate an integral using the trapezium rule with equally spaced x values.\n\ntrapezium <- function(v, a, b) {\n  n <- length(v) - 1\n  h <- (b - a) / n\n  intv <- (h / 2) * (sum(v) + sum(v[2 : n]))\n  intv\n}\n\nUse the above function to evaluate the integral \\(\\int_{1}^{5}arctan(x)dx\\) with 39 evaluation points (i.e. so that the input vector v has 39 elements).\n\ntrapezium(atan(seq(1, 5,length.out = 39)), 1, 5)\n\n\n\n\n\n\n\n\n\n\nCreate a QuarticMin() function to minimise function \\(h(x)=4x^2+5x+3\\), with starting value \\(x_0=-3\\).\n\nQuarticMin <- function(x0, tol = 1e-6, MaxIter = 100, Verbose = TRUE) {\n  x <- x0\n  Iter <- 0\n  dh.dx <- (8 * x) + 5 # First derivative\n  RelChange <- Inf\n  while (abs(dh.dx) > tol & abs(RelChange) > tol & Iter < MaxIter) {\n    if (Verbose) {\n      cat(paste(\"Iteration\",Iter,\":   Estimate =\",signif(x,5),\n                \"    Gradient = \",signif(dh.dx,5),\"\\n\"))\n    }\n    x.old <- x\n    d2h.dx2 <- 8 # Second derivative\n    x <- x - (dh.dx / d2h.dx2)\n    dh.dx <- (8 * x) + 5 # First derivative\n    RelChange <- (x-x.old) / x.old\n    Iter <- Iter + 1\n  }\n  if (Verbose) cat(\"---\\n\")\n  hmin <- (4 * x^2) + (5 * x) + 3 # Original\n  list(x=x, hx=hmin, gradient=dh.dx, N.iter=Iter)\n}\n\nQuarticMin(-3)\n\nNote: In Iteration n, Estimate = \\(x_{n}\\).\n\n\n\nCreate a tploglik() function to returns the negative log likelihood of a truncated Poisson distribution, up to a constant.\n\ntploglik <- function(theta, y) {\n  n <- length(y)\n  ybar <- mean(y)\n  n * (theta - (ybar * log(theta)) + log(1 - exp(-theta)))\n}\n\nTruncated Poisson Distribution Data\n\nTPdata <- rep(1:14, c(97, 164, 242, 182, 154, 83, 44, 19, 9, 7, 2, 0, 0, 1))\n\nUse nlm() and tploglik() functions to find the maximum likelihood estimate of \\(\\theta\\) together with its estimated standard error. Use the sample mean of the original 1004 values as the starting point for nlm().\n\nTP.fitted <- nlm(tploglik, mean(TPdata), y = TPdata, hessian = TRUE)\n\n\n## The MLE of theta\nTP.fitted$estimate\n## Estimated Standard Error of the MLE\n1 / sqrt(TP.fitted$hessian)\n\n\n\n\nData\n\nNLSdata <- read.table(file = \"nls2.dat\", header = TRUE)\n\nFit a linear model regressing log(Y) on x\n\nModel_Log <- lm(log(Y) ~ x, data = NLSdata)\nsummary(Model_Log)\n\nsumsqerr() function to compute sum of squared errors\n\nsumsqerr <- function(theta, x, Y) {\n  beta0 <- theta[1]\n  beta1 <- theta[2]\n  mu <- beta0 * exp(beta1 * x)\n  sum((Y - mu)^2)\n}\n\nFor your nonlinear model, what is the sum of squared errors for the parameter values \\(\\beta_0=1.4\\), \\(\\beta_1=-0.1\\)?\n\nsumsqerr(c(1.4, -0.1), x = NLSdata$x, Y = NLSdata$Y)\n\nTry fitting your nonlinear model using nlm(), with starting values \\(\\beta_0=-0.8\\) and \\(\\beta_1=-0.6\\). Compare the minimised sum of squares with the value that you obtained in the workshop starting from \\(\\beta_0=1.3\\) and \\(\\beta_1=-0.3\\). Has nlm() located (approximately) the same minimum this time?\n\nNLS.fit1 <- nlm(sumsqerr, c(-0.8, -0.6), x = NLSdata$x, Y = NLSdata$Y, hessian = TRUE)\nNLS.fit2 <- nlm(sumsqerr, c(1.3, -0.3), x = NLSdata$x, Y = NLSdata$Y, hessian = TRUE)\n\nTry fitting the model using nlm(), with starting values \\(\\beta_0=0.2\\) and \\(\\beta_1=0.4\\). At the estimated minimum, what is the (2, 2) element of the Hessian matrix of the sum of squares?\n\nNLS.fit3 <- nlm(sumsqerr, c(0.2, 0.4), x = NLSdata$x, Y = NLSdata$Y, hessian = TRUE)\nNLS.fit3$hessian[2, 2]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngalapagos.data <- read.table(\"galapagos.dat\")\n\n\n\n\ngalapagos.glm1 <- glm(Endemics ~ log(Area), \n                      family = poisson(link = \"log\"),\n                      data = galapagos.data)\nsummary(galapagos.glm1)\n\n\nCall:\nglm(formula = Endemics ~ log(Area), family = poisson(link = \"log\"), \n    data = galapagos.data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-4.0329  -1.9822  -0.3976   1.1263   3.8114  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  2.40127    0.06540   36.72   <2e-16 ***\nlog(Area)    0.27373    0.01192   22.96   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 743.55  on 29  degrees of freedom\nResidual deviance: 121.97  on 28  degrees of freedom\nAIC: 258.93\n\nNumber of Fisher Scoring iterations: 4\n\n\nIn this case, the model says that Y[i] (number of endemics for the ith island) comes from a Poisson distribution with mean mu[i], where\nlog(mu[i]) = beta0 + (beta1 * x[i])\nand x[i] is the log of the area of the ith island. The coefficients in the output are estimates of beta0 and beta1 respectively.\nNotice also the “Null deviance” (this is the deviance - or lack of fit - for a model containing just a constant term, analogous to the total sum of squares in a linear regression model), and the “Residual deviance” (the lack of fit for the model you’ve just fitted, analagous to the residual sum of squares). The residual deviance is much lower than the null deviance, suggesting that the model has “explained” a lot of the variation - just as you would say that a linear regression model explains a lot of the variation if the residual sum of squares is much lower than the total sum of squares.\nFinally, notice the AIC (Akaike Information Criterion). This is another measure of “model fit”, a really good model will have a high log-likelihood with few parameters. The lower the AIC, the better the model.\n\n\n\npar(mfrow = c(2,2), lwd = 2, mar = c(3,3,2,2), mgp = c(2,0.75,0))\nplot(galapagos.glm1, which = 1:4)\n\n\n\n\nThe Cook’s distance plot shows that there are some islands which do potentially have a big influence on the results; and there are also several residuals outside the range -2 to 2 (look at the “residuals vs fitted” plot).\nThis suggests that the endemic species counts are more variable than we would expect from a Poisson distribution. This phenomenon is called overdispersion. One possible explanation is that there are other factors in addition to island area that influence the number of endemic species. This is quite likely, in fact.\nThe next question is: what might those other factors be? And do we have the data?! The variables in the include “Nearest” (distance to the nearest other island in km) and “Adjacent” (the area of the nearest other island in km^2). It seems reasonable to imagine that if the nearest other island is very close by, then it is easy for species to move between islands and therefore that the number of endemics is likely to be lower. Let’s see whether there’s any evidence for that.\nTo help understand the model that you’ve just fitted, let’s pretend that all the assumptions are satisfied so that it makes sense to plot the fitted relationship - together with some confidence intervals, perhaps. Here goes:\n\nx.grid <- seq(min(galapagos.data$Area),\n              max(galapagos.data$Area), by = 0.1)\nEndemics.pred <- predict(galapagos.glm1,\n                         newdata = data.frame(Area = x.grid),\n                         se.fit = TRUE)\n\nz <- qnorm(0.975, mean = 0, sd = 1, lower.tail = TRUE)\nUL95 <- Endemics.pred$fit + (z*Endemics.pred$se.fit)\nLL95 <- Endemics.pred$fit - (z*Endemics.pred$se.fit)\n\npar(mfrow = c(1,1))\nplot(galapagos.data$Area, galapagos.data$Endemics, col = \"lightblue\",\n     pch = 15,log = \"x\",xlab = expression(\"Island area (km\"^2*\")\"),ylab = \"# endemics\",\n     main = \"Galapagos islands: numbers of endemic species\")\nlines(x.grid,exp(Endemics.pred$fit),col=\"blue\")\nlines(x.grid,exp(UL95),lty=2,col=\"blue\")\nlines(x.grid,exp(LL95),lty=2,col=\"blue\")\n\n\n\n\n\n\n\n\n\ngalapagos.glm2 <- update(galapagos.glm1, . ~ . + Nearest)\nsummary(galapagos.glm2)\n\n\nCall:\nglm(formula = Endemics ~ log(Area) + Nearest, family = poisson(link = \"log\"), \n    data = galapagos.data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-4.2260  -1.8476  -0.2745   1.4930   3.5398  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  2.460818   0.070985  34.667   <2e-16 ***\nlog(Area)    0.271698   0.011795  23.036   <2e-16 ***\nNearest     -0.004698   0.002400  -1.958   0.0503 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 743.55  on 29  degrees of freedom\nResidual deviance: 118.01  on 27  degrees of freedom\nAIC: 256.96\n\nNumber of Fisher Scoring iterations: 4\n\n\nAccording to the z test, the Nearest variable is not quite significant at 5% level.\nDo a chi-squared test to compare Model 1 and Model 2 :\n\nanova(galapagos.glm1, galapagos.glm2, test=\"Chi\")\n\nAnalysis of Deviance Table\n\nModel 1: Endemics ~ log(Area)\nModel 2: Endemics ~ log(Area) + Nearest\n  Resid. Df Resid. Dev Df Deviance Pr(>Chi)  \n1        28     121.97                       \n2        27     118.01  1   3.9643  0.04648 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe chi-squared test result suggests that we can reject the null hypothesis at the 5% level! Actually, the anova() results are usually more accurate than the z tests in the summary() output - however, in this particular instance everything seems too close to call.\nIt is also possible that the number of endemics is influenced by the size of the neighbouring island - but this probably won’t make much difference if that island is 200km away! This suggests adding the variable Adjacent to the model, as well as an interaction between Adjacent and Nearest.\n\n\n\n\ngalapagos.glm3 <- update(galapagos.glm2, . ~ . + Adjacent + Adjacent:Nearest)\nsummary(galapagos.glm3)\n\n\nCall:\nglm(formula = Endemics ~ log(Area) + Nearest + Adjacent + Nearest:Adjacent, \n    family = poisson(link = \"log\"), data = galapagos.data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-3.3091  -1.6579   0.0805   0.9147   2.9245  \n\nCoefficients:\n                   Estimate Std. Error z value Pr(>|z|)    \n(Intercept)       2.489e+00  7.206e-02  34.536  < 2e-16 ***\nlog(Area)         2.910e-01  1.290e-02  22.563  < 2e-16 ***\nNearest          -8.892e-03  3.005e-03  -2.959  0.00309 ** \nAdjacent         -3.635e-04  1.388e-04  -2.619  0.00881 ** \nNearest:Adjacent  4.447e-05  3.103e-05   1.433  0.15177    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 743.549  on 29  degrees of freedom\nResidual deviance:  91.898  on 25  degrees of freedom\nAIC: 234.85\n\nNumber of Fisher Scoring iterations: 5\n\n\nDo another chi-squared test to compare Model 2 and Model 3 :\n\nanova(galapagos.glm2, galapagos.glm3, test = \"Chi\")\n\nAnalysis of Deviance Table\n\nModel 1: Endemics ~ log(Area) + Nearest\nModel 2: Endemics ~ log(Area) + Nearest + Adjacent + Nearest:Adjacent\n  Resid. Df Resid. Dev Df Deviance  Pr(>Chi)    \n1        27    118.010                          \n2        25     91.898  2   26.112 2.137e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\npar(mfrow = c(2,2), lwd = 2, mar = c(3,3,2,2), mgp = c(2,0.75,0))\nplot(galapagos.glm3, which = 1:4)\n\n\n\n\nWell … there are still several residuals outside the range (-2,2). As a check on this, let’s calculate the variance of the Pearson residuals - which, for a Poisson model, are defined in such a way that if the model is correct their mean and variance should be roughly 0 and 1 respectively (see workshop notes). When calculating their variance, we need to account for the number of parameters estimated in the model - so we can’t just use the var() command directly.\n\nsum(resid(galapagos.glm3, type = \"pearson\")^2 ) / galapagos.glm3$df.residual\n\n[1] 3.433\n\n\nThat’s not even close to 1! Conclusion: we haven’t succeeded in explaining the excess variation. This means that all of the existing test results, confidence intervals and so forth will be incorrect because they are calculated under the assumption that the variance is equal to the mean. There isn’t much that we can do to improve the model further, but there is a trick that we can use to at least ensure that the test results are correct. This is to introduce a “dummy” dispersion parameter into the Poisson model, which just allows all of the standard errors to be increased to acknowledge the additional variability in the data.\nR allows this via the use of a “quasipoisson” family of distributions (there is also a “quasibinomial” family, that can be used for overdispersed binomial data). Be aware that there is no such thing in reality as a “quasiPoisson” or “quasibinomial” distribution: they are provided here solely because it is quite common to encounter overdispersion in applications, and the inclusion of a “dummy” dispersion parameter provides an easy (and legitimate!) way to ensure that standard errors etc. are correct. Here we go:\n\n\n\n\n\ngalapagos.glm4 <- update(galapagos.glm3, family = quasipoisson(link = \"log\"))\nsummary(galapagos.glm4)\n\n\nCall:\nglm(formula = Endemics ~ log(Area) + Nearest + Adjacent + Nearest:Adjacent, \n    family = quasipoisson(link = \"log\"), data = galapagos.data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-3.3091  -1.6579   0.0805   0.9147   2.9245  \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       2.489e+00  1.335e-01  18.640 3.54e-16 ***\nlog(Area)         2.910e-01  2.390e-02  12.177 5.24e-12 ***\nNearest          -8.892e-03  5.568e-03  -1.597    0.123    \nAdjacent         -3.635e-04  2.572e-04  -1.414    0.170    \nNearest:Adjacent  4.447e-05  5.749e-05   0.774    0.446    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 3.433)\n\n    Null deviance: 743.549  on 29  degrees of freedom\nResidual deviance:  91.898  on 25  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 5\n\n\nNotice that the AIC is reported as NA. This is because the AIC requires a log-likelihood, and the log-likelihood is computed from the probability density or probability mass function of the fitted model, and there is no such thing as “the probability mass function of a quasiPoisson distribution”. You might reasonably ask “how can you estimate the parameters by maximum likelihood then?” The answer - at least, for the purposes of this course - is: the parameters are estimated as though the data follow a Poisson distribution, and then the “dummy” dispersion parameter is introduced afterwards.\nWe can’t use the anova() command to compare the quasipoisson model with any of the earlier models: it can only be used to compare nested models within the same distributional “family”. However, if you call anova() with just a single model then it carries out a sequence of tests as though you built up the model in stages, at each stage adding one extra term. Like this (notice the use of test=\"F\", because we now have a dispersion parameter).\n\nanova(galapagos.glm4, test = \"F\")\n\nAnalysis of Deviance Table\n\nModel: quasipoisson, link: log\n\nResponse: Endemics\n\nTerms added sequentially (first to last)\n\n                 Df Deviance Resid. Df Resid. Dev        F    Pr(>F)    \nNULL                                29     743.55                       \nlog(Area)         1   621.57        28     121.97 181.0586 5.941e-13 ***\nNearest           1     3.96        27     118.01   1.1547   0.29282    \nAdjacent          1    24.07        26      93.94   7.0106   0.01383 *  \nNearest:Adjacent  1     2.05        25      91.90   0.5957   0.44746    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlogdose <- c(1.6907, 1.7242, 1.7552, 1.7842, 1.8113, 1.8369, 1.8610, 1.8839)\ntotals <- c(59, 60, 62, 56, 63, 59, 62, 60)\nkilled <- c(6, 13, 18, 28, 52, 53, 61, 60)\nproportions <- killed / totals\n\n\n\n\nNotice the “weights” argument: When using glm() with binomial data, this is needed to specify the values of “n” for each observation (if you think about it, an observed proportion from a group containing n=1000 insects will be more accurate than a proportion from a group containing just n=10 insects.\n\nbeetle.glm <- glm(proportions ~ logdose, weights = totals, \n                  family = binomial(link = \"logit\"))\nsummary(beetle.glm)\n\n\nCall:\nglm(formula = proportions ~ logdose, family = binomial(link = \"logit\"), \n    weights = totals)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.5941  -0.3944   0.8329   1.2592   1.5940  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -60.717      5.181  -11.72   <2e-16 ***\nlogdose       34.270      2.912   11.77   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 284.202  on 7  degrees of freedom\nResidual deviance:  11.232  on 6  degrees of freedom\nAIC: 41.43\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n\npar(mfrow = c(2,2), lwd = 2, mar = c(3,3,2,2), mgp = c(2,0.75,0))\nplot(beetle.glm, which = 1:4)\n\n\n\n\n\n\n\n\n\nbeetle.glm.probit <- glm(proportions ~ logdose, weights = totals,\n                         family=binomial(link=\"probit\") )\nsummary(beetle.glm.probit)\n\n\nCall:\nglm(formula = proportions ~ logdose, family = binomial(link = \"probit\"), \n    weights = totals)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.5714  -0.4703   0.7501   1.0632   1.3449  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -34.935      2.648  -13.19   <2e-16 ***\nlogdose       19.728      1.487   13.27   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 284.20  on 7  degrees of freedom\nResidual deviance:  10.12  on 6  degrees of freedom\nAIC: 40.318\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n\npar(mfrow = c(2,2), lwd = 2, mar = c(3,3,2,2), mgp = c(2,0.75,0))\nplot(beetle.glm.probit, which = 1:4)\n\n\n\n\n\n\n\n\n\nbeetle.glm.cloglog <- glm(proportions ~ logdose, weights = totals,\n                         family=binomial(link=\"cloglog\") )\nsummary(beetle.glm.cloglog)\n\n\nCall:\nglm(formula = proportions ~ logdose, family = binomial(link = \"cloglog\"), \n    weights = totals)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-0.80329  -0.55135   0.03089   0.38315   1.28883  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -39.572      3.240  -12.21   <2e-16 ***\nlogdose       22.041      1.799   12.25   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 284.2024  on 7  degrees of freedom\nResidual deviance:   3.4464  on 6  degrees of freedom\nAIC: 33.644\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n\npar(mfrow = c(2,2), lwd = 2, mar = c(3,3,2,2), mgp = c(2,0.75,0))\nplot(beetle.glm.cloglog, which = 1:4)\n\n\n\n\nSeems that Model 3 is better than the other two.\nProduce a plot of the fitted dose-response curve with some confidence intervals using Model 3:\n\nx.grid <- seq(1.6, 1.9, 0.01)\nBeetle.pred <- predict(beetle.glm.cloglog, newdata = data.frame(logdose = x.grid), se.fit = TRUE)\n\nz <- qnorm(0.975, mean = 0,sd = 1,lower.tail = TRUE)\nUL95 <- Beetle.pred$fit + (z * Beetle.pred$se.fit)\nLL95 <- Beetle.pred$fit - (z * Beetle.pred$se.fit)\n\npar(mfrow=c(1,1))\nplot(logdose, proportions,\n     xlab = expression(log[10]*\" CS\"[2]*\" concentration (mg\\ l\"^{-1}*\")\"),\n     ylab = \"Proportion\",main = \"Proportion of insects killed\",\n     col = \"salmon\",pch = 15)\n\nlines(x.grid, 1 - exp(-exp(Beetle.pred$fit)), col=\"red\")\nlines(x.grid, 1 - exp(-exp(UL95)), lty=2, col=\"red\")\nlines(x.grid, 1 - exp(-exp(LL95)), lty=2, col=\"red\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nYearlyAttacks <- read.csv(\"terrorism.dat\", header = TRUE, sep = \";\")\nCountryData <- read.csv(\"countries.dat\", header = TRUE, sep = \";\")\nYearlyAttacks <- merge(YearlyAttacks, CountryData, all = TRUE)\nYearlyAttacks <- YearlyAttacks[order(YearlyAttacks$Year, YearlyAttacks$Country),] # Reorder the data\n\nLook at the USA data:\n\nwanted.rows <- grepl(\"United States\",YearlyAttacks$Country, fixed = TRUE)\nUSAttacks <- YearlyAttacks[wanted.rows,]\n\npar(lwd = 2, mar = c(3,3,2,2), mgp = c(2,0.75,0))\nplot(USAttacks$Year, USAttacks$Freq, type = \"l\",col = \"red\",\n     xlab = \"Year\",ylab = \"Number of attacks\",\n     main = \"Terrorist attacks in the USA, 1968-2009\")\n\n\n\n\n\n\n\nUStrends <- gam(Freq ~ s(Year), family = poisson(link = \"log\"),\n                data = USAttacks)\nsummary(UStrends)\n\n\nFamily: poisson \nLink function: log \n\nFormula:\nFreq ~ s(Year)\n\nParametric coefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   2.3032     0.0573    40.2   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n          edf Ref.df Chi.sq p-value    \ns(Year) 7.946  8.693  215.5  <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.514   Deviance explained =   67%\nUBRE = 2.8064  Scale est. = 1         n = 42\n\n\nThat’s not very informative, except that it tells us that the smooth function of “Year” is highly significant (let’s face it, this was obvious already!). Notice the use of s(Year) to represent a “smooth function of Year”. The edf in the summary() output is “effective degrees of freedom”, which is a measure of the complexity of the fitted curve.\nCheck for overdispersion:\n\nsum(resid(UStrends,type = \"pearson\")^2) / UStrends$df.residual\n\n[1] 4.193033\n\n\nNot close to 1! Let’s do a quasipoisson fit, therefore:\n\n\n\n\nUStrends2 <- update(UStrends, family = quasipoisson(link = \"log\"))\nsummary(UStrends2)\n\n\nFamily: quasipoisson \nLink function: log \n\nFormula:\nFreq ~ s(Year)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   2.3542     0.1115   21.12   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n          edf Ref.df     F p-value    \ns(Year) 6.189  7.325 6.517 5.1e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.527   Deviance explained = 65.2%\nGCV = 5.1928  Scale est. = 4.4196    n = 42\n\n\nIn contrast to the GLM example for the Galapagos endemics, you’ll notice here that the coefficients in the summary() output do change a bit. This is related to the way that the smoothing parameter is selected automatically. Almost all of the strange things about GAMs are related to this, in fact.\nPlot the estimates of the smooth function:\n\nplot(UStrends)\n\n\n\n\nIf we really want to show what the model tells us about trends in US terror attacks, we can do it using the predict() command, in exactly the same way that we did for the Galapagos endemics example. Like this:\n\neta.predicted <- predict(UStrends2, se.fit = TRUE)\nz <- qnorm(0.975, mean = 0, sd = 1, lower.tail = TRUE)\nUL95 <- eta.predicted$fit + (z * eta.predicted$se.fit)\nLL95 <- eta.predicted$fit - (z * eta.predicted$se.fit)\nplot(USAttacks$Year, USAttacks$Freq, type = \"p\",col = \"red\",pch=15,\n     xlab = \"Year\",ylab = \"Number of attacks\",\n     main = \"Terrorist attacks in the USA, 1968-2009\")\nlines(USAttacks$Year, exp(eta.predicted$fit), col = \"darkred\")\nlines(USAttacks$Year, exp(UL95), col = \"darkred\", lty = 2)\nlines(USAttacks$Year, exp(LL95), col = \"darkred\", lty = 2)"
  },
  {
    "objectID": "UCL-Schoolwork/ECON0047.html",
    "href": "UCL-Schoolwork/ECON0047.html",
    "title": "From Jupyter",
    "section": "",
    "text": "You can interact with Quarto through JupyterLab or JupyterHub. Your Jupyter setup will involve .ipynb notebooks and the command line. Quarto’s JupyterLab tutorials has great instructions on getting started with JupyterLab, including computations and authoring.\nHere we will demonstrate how to work with this Quarto tutorial site in JupyterHub and add a Jupyter Notebook (.ipynb file). This example uses the NASA-Openscapes JupyterHub that already has all python environments as well as Quarto installed."
  },
  {
    "objectID": "UCL-Schoolwork/ECON0047.html#setup",
    "href": "UCL-Schoolwork/ECON0047.html#setup",
    "title": "From Jupyter",
    "section": "Setup",
    "text": "Setup\n\nJupyterHub\nOur JupyterHub is already setup with python environments as well as Quarto (through nasa-openscapes/corn), so there is no further installation required.\n\n\nClone your repo\nYou’ll start by cloning your repository into JupyterHub. Do this by opening a terminal (File > New > Terminal). In the Terminal, git clone your repository and cd into it:\ngit clone https://github.com/openscapes/quarto-website-tutorial\ncd quarto-website-tutorial\n\n\nInstall Quarto\nNot needed - Quarto is already installed on the NASA-Openscapes JupyterHub! But to install elsewhere you would do so from https://quarto.org/docs/get-started/.\nQuarto is a Command Line Interface (CLI), like git. Once download is complete, follow the installation prompts on your computer like you do for other software. You won’t see an application to click on when it is installed.\nNote for Mac users: If you do not have administrative privileges, please select “Install for me only” during the Destination Selection installation step (you will first click on “Change Install Location” at the Installation Type step).\nYou can check to confirm that Quarto is installed properly from the command line:\nquarto check install\n\n\n\n\n\n\nAdditional checks\n\n\n\n\n\nYou can also run:\n\nquarto check knitr to locate R, verify we have the rmarkdown package, and do a basic render\nquarto check jupyter to locate Python, verify we have Jupyter, and do a basic render\nquarto check to run all of these checks together\n\n\n\n\n\n\n\n\n\n\nHistorical aside: Install Quarto in a docker container\n\n\n\n\n\nIn Summer 2021 some NASA Mentors trying to install quarto locally was not an option, but they were able to install it inside a container using the following Dockerfile:\n#| fold: true\n#| summary: \"Show the Dockerfile\"\n\n##############################\n# This Dockerfile installs quarto and then runs quarto serve against the\n# internal /home/quarto/to_serve.\n#\n# BUILD\n# -----\n# To build this container, run\n#\n#     docker build -t quarto_serve .\n#\n# Add the --no-cache option to force docker to build fresh and get the most\n# recent version of quarto.\n#\n#\n# RUN\n# ---\n# 1. Find the directory you want quarto to serve. Let's call this /PATH/TO/earthdata-cloud-cookbook.\n# 2. Run docker:\n#\n#     docker run --rm -it -p 4848:4848 -v /PATH/TO/earthdata-cloud-cookbook:/home/quarto/to_serve quarto_serve\n#\n# 3. Open your browser and go to http://127.0.0.1:4848/\n#\n##############################\n\nFROM ubuntu:hirsute\n\n######\n# Install some command line tools we'll need\n######\nRUN apt-get update\nRUN apt-get -y install wget\nRUN apt-get -y install gdebi-core\nRUN apt-get -y install git\n\n\n######\n# Install quarto (https://quarto.org/)\n######\n\n# This is a quick and dirty way of getting the newest version number from\n# https://github.com/quarto-dev/quarto-cli/releases/latest. What's happening is\n# we're pulling the version number out of the redirect URL. This will end up\n# with QVER set to something like 0.2.11.\nRUN QVER=`wget --max-redirect 0 https://github.com/quarto-dev/quarto-cli/releases/latest 2>&1 | grep \"Location\" | sed 's/L.*tag\\/v//' | sed 's/ .*//'` \\\n    && wget -O quarto.deb \"https://github.com/quarto-dev/quarto-cli/releases/download/v$QVER/quarto-$QVER-amd64.deb\"\nRUN gdebi -n quarto.deb\n\n# Run this to make sure quarto installed correctly\nRUN quarto check install\n\n\n######\n# Create a non-root user called quarto\n######\nRUN useradd -ms /bin/bash quarto\nUSER quarto\nRUN mkdir /home/quarto/to_serve\nWORKDIR /home/quarto/to_serve\n\n\n######\n# Start quarto serve\n######\n\nCMD quarto serve --no-browse --host 0.0.0.0 --port 4848"
  },
  {
    "objectID": "UCL-Schoolwork/ECON0047.html#quarto-preview",
    "href": "UCL-Schoolwork/ECON0047.html#quarto-preview",
    "title": "From Jupyter",
    "section": "Quarto preview",
    "text": "Quarto preview\nLet’s start off by previewing our quarto site locally. In Terminal, type quarto preview, which will provide a URL with a preview of our site!\nquarto preview\n# Preparing to preview\n# Watching files for changes\n# Browse at https://openscapes.2i2c.cloud/user/jules32/proxy/4593/\nCopy this URL into another browser window; and arrange them so you can see them both. I make a bit more space in Jupyter by collapsing the left file menu by clicking on the file icon at the top of the left sidebar.\n\n\n\n\n\n\nMake a small change and preview it\nNow we’ll be able to see live changes in the preview as we edit in our .md files. Let’s try it: Change the date in index.md by opening it from the file directory. Change to today’s date, and save. Your preview window will refresh automatically! If it does not, you can also refresh the page manually. The refreshed previewed site will now display your changes!"
  },
  {
    "objectID": "UCL-Schoolwork/ECON0047.html#create-a-new-.ipynb-page",
    "href": "UCL-Schoolwork/ECON0047.html#create-a-new-.ipynb-page",
    "title": "From Jupyter",
    "section": "Create a new .ipynb page",
    "text": "Create a new .ipynb page\nLet’s add a new page to our site. Instead of an .md file like the others, let’s add a .ipynb file.\nFile > New > Notebook. Accept the default kernel by clicking Select.\n\nFirst chunk: raw yaml\nBy default, this Notebook will give us a first chunk that is code. Let’s change it to raw so that we can write our yaml at the top.\n\n\n\n\n\nIn our Raw code chunk, let’s write the title of this document. We need three dashes --- on separate lines preceding and following the title:, which you can name as you’d like.\n---\ntitle: Python Example\n---\n\n\nSecond chunk: Markdown\nLet’s add a new chunk that is Markdown so we can write some description of what this page will be.\nClick the + symbol at the top of the document, and this will add a new chunk, which by default again is a Code chunk. Change it to a Markdown Chunk following the steps we did above when switching to Raw.\nHere, write a little bit of text in Markdown. Since your title is effectively a level-1 header, avoid using level-1 headers in the rest of your document. Here is some example text I wrote:\n## Introduction\n\nThis example has some Python code that will be a part of our Quarto site.\n\n\nThird chunk: Code\nNow let’s create a new chunk with the default Code setting.\nPaste the following code (or write some of your own to test):\n#| label: fig-polar\n#| fig-cap: \"A line plot on a polar axis\"\nimport numpy as np\nimport matplotlib.pyplot as plt\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\nNow, go ahead and execute this code chunk like you normally would, by clicking the cursor in a code block and clicking the sideways “play” triangle to run the selected cells (and advance to the next cell). This code produces a plot.\nNote that the code runs as it normally would; the code options in the comments are just comments.\n\n\nSave your file\nSave your document - I’ll call mine python-example.ipynb in the main repository."
  },
  {
    "objectID": "UCL-Schoolwork/ECON0047.html#update-_quarto.yml",
    "href": "UCL-Schoolwork/ECON0047.html#update-_quarto.yml",
    "title": "From Jupyter",
    "section": "Update _quarto.yml",
    "text": "Update _quarto.yml\nNow we’ll add python-example.ipynb to our _quarto.yml file; this is where we register of all files to include in our site. Let’s add it after the section called “Basic Workflows”.\nOpen _quarto.yml by clicking on it from the file directory.\nScroll down to review the current contents in the sidebar: section. It’s there we see all the file arrangement that we see in the previewed site.\nAdd - python-example.ipynb to line 46, making sure that your indentation aligns with the other pages.\n\n\n\n\n\nYou’ll see that our new page shows up in our Preview, and the code is executed since we did that in the Jupyter Notebook itself. By default, Quarto will not execute code chunks since your computations will likely become more complex and you will want to control when they are executed (or “run”).\nSince Quarto is still previewing our website and the python-example.ipynb, the plot also displays in the notebook after the code is run and the file is saved, as shown below.\n\n\n\n\n\nSo, your normal workflow for creating and running code blocks in your Jupyter Notebook is the same one you’ll use as Quarto displays the preview."
  },
  {
    "objectID": "UCL-Schoolwork/ECON0047.html#quarto-render",
    "href": "UCL-Schoolwork/ECON0047.html#quarto-render",
    "title": "From Jupyter",
    "section": "Quarto render",
    "text": "Quarto render\nSo far we have used Quarto preview to view our website as we develop it. Quarto render will build the html elements of the website that we can see when we preview. Rendering will format the markdown text and code nicely as a website (or however is indicated in the _quarto.yml).\nBy default, Quarto render does not execute code in a Jupyter notebook. It will never run .ipynb files unless you tell it to.\n\nRender whole notebook\nIf you would like it to specifically execute code in a Jupyter notebook, you can do so in Terminal.\nOur Terminal is still busy previewing our website, so let’s open a new Terminal.\nFile > New > Terminal. Then type:\ncd quarto-website-tutorial\nquarto render python-example.ipynb --execute"
  },
  {
    "objectID": "UCL-Schoolwork/ECON0047.html#authoring-tips",
    "href": "UCL-Schoolwork/ECON0047.html#authoring-tips",
    "title": "From Jupyter",
    "section": "Authoring tips",
    "text": "Authoring tips\nQuarto.org has details about authoring, including specific instructions about authoring in Jupyter: quarto.org/docs/reference/cells/cells-jupyter."
  },
  {
    "objectID": "UCL-Schoolwork/ECON0047.html#commit-and-push",
    "href": "UCL-Schoolwork/ECON0047.html#commit-and-push",
    "title": "From Jupyter",
    "section": "Commit and push!",
    "text": "Commit and push!\nCommitting and pushing will make the changes you see locally live on your website (using the GitHub Action we set up earlier)."
  },
  {
    "objectID": "UCL-Schoolwork/ECON0047.html#troubleshooting",
    "href": "UCL-Schoolwork/ECON0047.html#troubleshooting",
    "title": "From Jupyter",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nMy changes don’t show up in preview\nMake sure you’ve saved your file! There might be a slight delay depending on your JupyterHub/Lab setup.\n\n\nQuarto render hangs / does not complete\nCheck the specific notebook, are there any `—` throughout to denote line breaks rather than yaml? They might be causing the issue; consider deleting those.\nAlso check how long the first raw cell is. Are there level-1 headers (#)? Try removing them."
  },
  {
    "objectID": "UCL-Schoolwork/STAT0024.html",
    "href": "UCL-Schoolwork/STAT0024.html",
    "title": "From RStudio",
    "section": "",
    "text": "The RStudio software (called an IDE, integrated development environment) is an excellent way to edit files and interface with GitHub. Plus, as it is made by the same folks who make Quarto, it has many integrated features for streamlining your workflow with Quarto, including how it previews your edits and provides debugging support for yaml! Quarto's RStudio tutorials has great instructions on getting started with RStudio, including computations and authoring.\nHere is what you’ll need to do to set up and use RStudio with Quarto."
  },
  {
    "objectID": "UCL-Schoolwork/STAT0024.html#setup",
    "href": "UCL-Schoolwork/STAT0024.html#setup",
    "title": "From RStudio",
    "section": "Setup",
    "text": "Setup\n\nRStudio and GitHub\nFor a workflow with RStudio and GitHub on your local computer, you will need four things:\n\nR\nRStudio\nGit\nGitHub\n\nFollow the UCSB MEDS Installation Guide for detailed instructions on how to create accounts, download, install, and configure on Mac and Windows. This takes about 20 minutes. (For an even more detailed walk-through, see Allison Horst’s ESM 206 Google Doc).\n\n\nClone your repo\nYou’ll start by cloning your repository into RStudio.\nFile > New Project > Version Control > Git > paste your repository name.\nR for Excel Users: Clone your repository using RStudio has detailed instructions and screenshots of these steps.\n\n\nInstall Quarto\nNext, you’ll install Quarto: https://quarto.org/docs/get-started/. After downloading, follow the installation wizard on your computer. When it is complete, you won’t see an application or any new software, but it is now available to RStudio (as well as all other applications on your computer, including the command line).\n\n\nRStudio orientation\nNow let’s take a moment to get oriented. This is an RStudio project, which is indicated in the top-right. The bottom right pane shows all the files in your project; everything we’ve cloned from GitHub. We can open any RStudio project by opening its .Rproj file, or from RStudio File > Open Project ….\n\n\n\nRStudio IDE highlighting the project name and files pane\n\n\n\n\nVisual Editor\nThe RStudio Visual Editor is quite new and has features that improve your writing experience. Working in the Visual Editor feels a bit like working in a Google Doc.\nHere’s an example showing the same file in the original Source Editor with content in markdown format and in the Visual Editor with content that looks more like it will appear in a live site. You can switch freely between these modes.\n\n\n\n\n\n\nRStudio IDE highlighting the Source Editor\n\n\n\n\n\n\n\nRStudio IDE highlighting the Visual Editor\n\n\n\n\n\nAlready have some content formatted in a Google Doc? You can copy-paste it into the Visual Editor and most formatting will be retained.\nThe editing bar provides familiar point and click access to text formatting options like bulleted or numbered lists.\n\n\n\nRStudio IDE highlighting the point and click editing bar\n\n\n\nKeyboard shortcuts\nThe Visual Editor also lets you use many keyboard shortcuts that might be familiar for adding boldface (command-b), italics (command-i), or headers. On a Mac, option-command-2 will make a level 2 header. Try it with option-command-1, or option-command-0 for normal text!\n\n\nInsert an image or figure\nTo insert an image (called a figure in Quarto), click the image icon. This brings up a window in which we can select the image, set its alignment, give it a caption and alt text, hyperlink it, or edit other metadata.\n\n\n\nInsert image or figure using the Visual Editor\n\n\nOnce an image is added, clicking on that image gives us editing options. We can resize it dynamically by clicking in the image and dragging a corner or side to resize. When an image is selected, its dimensions are displayed for editing. Clicking on the gray ellipsis to the right of the dimensions opens the pop-up window to access more metadata edits.\n\n\nInsert a table\nSimilar to adding an image, to insert a table, we click the Table dropdown."
  },
  {
    "objectID": "UCL-Schoolwork/STAT0024.html#quarto-render",
    "href": "UCL-Schoolwork/STAT0024.html#quarto-render",
    "title": "From RStudio",
    "section": "Quarto render",
    "text": "Quarto render\nIn the Build tab in the top-right pane, click “Render Website”. This will build the .html files and preview your website. It’s equivalent to “knitting” in RMarkdown.\nNote that you can also click “Preview Website”. With “Render Website” in RStudio, Quarto is able to render and preview in one step.\nIf you’d ever like to stop the preview, in the bottom-left, click on the Jobs tab and then the red Stop button.\n\nMake a small change and render it\nClick on index.md. This will open this markdown file in a fourth pane; the editor pane. Make a small change, for example change to today’s date on Line 4. Then, save your file; there is a disc icon at the top of the file.\nThen, render this file: press “Render” which is to the right of the disc icon that saves the file. This will render only this single file, as opposed to rerendering the whole website like when we clicked “Render Website” in the top right pane. Checking Render on Save (between the disc icon and the Render button) is a great strategy for doing this in one step."
  },
  {
    "objectID": "UCL-Schoolwork/STAT0024.html#create-a-new-.rmd-page",
    "href": "UCL-Schoolwork/STAT0024.html#create-a-new-.rmd-page",
    "title": "From RStudio",
    "section": "Create a new .Rmd page",
    "text": "Create a new .Rmd page\nNew > RMarkdown document > OK\nThe starter RMarkdown document has some R code inside: it displays a summary of the cars dataset that is pre-loaded into R (summary(cars)) and plots the pressure data that is also pre-loaded (plot(pressure)).\nSave this document as r-example.rmd."
  },
  {
    "objectID": "UCL-Schoolwork/STAT0024.html#update-_quarto.yml",
    "href": "UCL-Schoolwork/STAT0024.html#update-_quarto.yml",
    "title": "From RStudio",
    "section": "Update _quarto.yml",
    "text": "Update _quarto.yml\nNow we’ll add r-example.rmd to our _quarto.yml file; this is where we register all files to include in our site. Let’s add it after the section called “Quarto Workflows”.\nOpen _quarto.yml by clicking on it from the file directory.\nScroll down to review the current contents in the sidebar: section under contents:. It’s there we see all the file arrangement that we see in the previewed site.\nAdd - r-example.rmd in its own line, making sure that your indentation aligns with the other pages.\nFrom the Build tab, clicking Preview Website will recreate your website!"
  },
  {
    "objectID": "UCL-Schoolwork/STAT0024.html#authoring-tips",
    "href": "UCL-Schoolwork/STAT0024.html#authoring-tips",
    "title": "From RStudio",
    "section": "Authoring tips",
    "text": "Authoring tips\nChecking “Render on Save” is really helpful when iterating quickly on a document.\nIf the document is very code-heavy, consider using freeze that will not run the code each time.\nQuarto.org has details about authoring, including specific instructions about authoring in RStudio."
  },
  {
    "objectID": "UCL-Schoolwork/STAT0024.html#commit-and-push",
    "href": "UCL-Schoolwork/STAT0024.html#commit-and-push",
    "title": "From RStudio",
    "section": "Commit and push!",
    "text": "Commit and push!\nCommitting and pushing will make the changes you see locally live on your website (using the GitHub Action we set up earlier)."
  },
  {
    "objectID": "UCL-Schoolwork/STAT0024.html#troubleshooting",
    "href": "UCL-Schoolwork/STAT0024.html#troubleshooting",
    "title": "From RStudio",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nIf you have trouble rendering your website after for example changing the extenstion of a file from .md to .qmd, refreshing your RStudio often helps. Do this by clicking the project name at the upper right of the RStudio window (in this case, quarto-website-tutorial), and underneath the “close project” section, click the same name of your project: quarto-website-tutorial. This will relaunch your whole project afresh."
  },
  {
    "objectID": "UCL-Schoolwork/MATH0050.html",
    "href": "UCL-Schoolwork/MATH0050.html",
    "title": "MATH0050 Review Note",
    "section": "",
    "text": "The symbols which we will use to construct our language consist of:"
  },
  {
    "objectID": "Tools-and-Tips.html#mac",
    "href": "Tools-and-Tips.html#mac",
    "title": "Tools and Tips",
    "section": "Mac",
    "text": "Mac\n\niCloud\nIf iCloud gets stuck during the uploading process (never finish uploading), open the terminal and try the following:\n\n\nTerminal\n\nkillall bird\nkillall cloudd\n\nThe codes above will terminate the core process of file synchronization, and the process will start again automatically after a few seconds."
  },
  {
    "objectID": "Tools-and-Tips.html#apple-products",
    "href": "Tools-and-Tips.html#apple-products",
    "title": "Tools and Tips",
    "section": "Apple Products",
    "text": "Apple Products\n\nMac\n\niCloud\nIf iCloud gets stuck during the uploading process (never finish uploading), open the terminal and try the following:\n\n\nTerminal\n\nkillall bird\nkillall cloudd\n\nThe codes above will terminate the core process of file synchronization, and the process will start again automatically after a few seconds.\n\n\n\niPhone\n\n截屏选择”拷贝并删除”后，键盘卡死\n只需要再随意截屏一次即可，上一张拷贝了的截屏仍然会在剪贴板里，且可正常粘贴。"
  },
  {
    "objectID": "UCL-Schoolwork/STAT0011.html",
    "href": "UCL-Schoolwork/STAT0011.html",
    "title": "STAT0011",
    "section": "",
    "text": "You are planning a vacation in Italy. Before packing, you hear that there might be an earthquake the day you arrive.\nAfter consulting Google, you learn that in recent years there have been (on average) five earthquakes a year in the part of the country you are visiting (ignore leap years). Moreover, you learn that when there is an earthquake, the earthquake forecast service has correctly predicted it 90% of the time. However, when there was no earthquake, the forecast service incorrectly predicted 10% of the time that there would be one.\nWhat is the probability that there will be an earthquake on the day you arrive given forecast of an earthquake?\n\n\n\n\nConsider a woman who has a brother with haemophilia, but whose father does not have haemophilia. This implies that her mother must be a carrier of the haemophilia gene on one of her X chromosomes and that her father is not a carrier. The woman herself thus has a fifty-fifty chance of having the gene.\nThe situation involving uncertainty is whether or not the woman carries the haemophilia gene. The parameter of interest θ can take two states:\n\nCarries the gene (θ = 1)\nDoes not carry the gene (θ = 0)\n\n\nWrite down the prior distribution for θ using the above information.\n\nThe data Y is the number of the woman’s sons who are infected. Suppose she has two sons, neither of whom is affected. Assuming the status of the two sons is independent, write down the likelihood function p(Y|θ) (if the woman is not a carrier then her sons cannot be affected, but if she is a carrier they each have a 50% chance of being effected)."
  },
  {
    "objectID": "UCL-Schoolwork/STAT0011.html#gamma-distribution",
    "href": "UCL-Schoolwork/STAT0011.html#gamma-distribution",
    "title": "STAT0011",
    "section": "Gamma Distribution",
    "text": "Gamma Distribution"
  },
  {
    "objectID": "UCL-Schoolwork/STAT0011.html#uniform-distribution",
    "href": "UCL-Schoolwork/STAT0011.html#uniform-distribution",
    "title": "STAT0011",
    "section": "Uniform Distribution",
    "text": "Uniform Distribution"
  },
  {
    "objectID": "UCL-Schoolwork/STAT0011.html#continuous-distributions",
    "href": "UCL-Schoolwork/STAT0011.html#continuous-distributions",
    "title": "STAT0011",
    "section": "Continuous Distributions",
    "text": "Continuous Distributions\n\nNormal Distribution (Gaussian Distribution)\n\n\n\nUniform Distribution\n\n\n\nGamma Distribution"
  },
  {
    "objectID": "UCL-Schoolwork/STAT0011.html#week-5-data",
    "href": "UCL-Schoolwork/STAT0011.html#week-5-data",
    "title": "STAT0011",
    "section": "Week 5 Data",
    "text": "Week 5 Data\nRequired Packages\n\nsuppressPackageStartupMessages({\n  library(VineCopula) # Copula analysis\n  library(ADGofTest) # Anderson-Darling Goodness-of-Fit test\n  library(KScorrect) # (Lilliefors-Corrected) Kolmogorov-Smirnov Goodness-of-Fit test\n  library(fGarch) # Time series analysis\n})\n\nLoad the data and have a look:\n\nload(\"week5data.RData\")\nplot(dataset)\n\n\n\n\nThis does not look like a sample from a bivariate normal distribution! In fact, ret1 follows a Student-t distribution with 10 degrees of freedom (df); ret2 follows a Student-t distribution with 6 df. Let’s assume we have this information.\nNext we apply Probability Integral Transform (PIT) to ret1 which has Student-t distribution with 10 df:\n\nu1 <- pstd(dataset$ret1, mean = 0, sd = 1, nu = 10)\nhist(u1)\n\n\n\n\nImplement Kolmogorov-Smirnov test:\n\nKStest1 <- LcKS(u1, cdf = \"punif\")\nKStest1$p.value\n\n[1] 0.377\n\n\nImplement Anderson-Darling test:\n\nADtest1 <- ad.test(u1, null = \"punif\")\nADtest1$p.value\n\n       AD \n0.3243088 \n\n\nThen we apply Probability Integral Transform (PIT) to ret2 which has Student-t distribution with 6 df:\n\nu2 <- pstd(dataset$ret2, mean = 0, sd = 1, nu = 6)\nhist(u2)\n\n\n\n\nImplement Kolmogorov-Smirnov test:\n\nKStest2 <- LcKS(u2, cdf = \"punif\")\nKStest2$p.value\n\n[1] 0.2062\n\n\nImplement Anderson-Darling test:\n\nADtest2 <- ad.test(u1, null = \"punif\")\nADtest2$p.value\n\n       AD \n0.3243088 \n\n\nWe pass the test for uniformity for both transformed log-returns, so we can proceed to copula modelling.\nUsing BiCopSelect function fit various copulas to the dataset and select the copula that provides the best fit based on the AIC criterion:\n\nplot(cbind(u1, u2))\n\n\n\nmodel <- BiCopSelect(u1, u2, familyset = NA, selectioncrit = \"AIC\", indeptest = TRUE, level = 0.05)\nmodel\n\nBivariate copula: Clayton (par = 1.94, tau = 0.49) \n\n\nThe model of best fit is Clayton with the estimated parameter theta = 1.94.\nNext we estimate the Value-at-Risk using the Monte Carlo simulation approach based on copula theory:\n\nN <- 1800\nset.seed(0123)\nu_sim <- BiCopSim(N, family = 3, model$par) # family = 3 means Clayton\n\nNext we apply the component-wise Inverse Probability Integral Transform (IPIT) to both ret1 and ret2.\n\nret1_sim <- qstd(u_sim[,1], mean = 0, sd = 1, nu = 10)\nret2_sim <- qstd(u_sim[,2], mean = 0, sd = 1, nu = 6)\n\nNote that our selected copula model is capable of generating values of log-returns with the observed dependence structure:\n\npar(mfrow=c(2,1))\nplot(dataset, ylab = \"ret2\", xlab = \"ret1\", col = \"blue\", main = \"Original log-returns\")\nplot(data.frame(ret1_sim, ret2_sim), ylab = \"ret2_sim\", xlab = \"ret1_sim\",\n     col = \"blue\", main = \"Simulated log-returns\")\n\n\n\npar(mfrow=c(1,1))\n\nNext, compute portfolio log-returns:\n\nport_sim <- matrix(0, nrow = N, ncol = 1)\nVaR_sim <- matrix(0, nrow = 1, ncol = 2)\n\nport_sim <- log(1 + ((exp(ret1_sim) - 1) + (exp(ret2_sim) - 1)) * (1 / 2)) # Need to transform individual log-returns back to individual net returns, and compute portfolio net returns, finally compute portfolio log-returns.\n\nReturn the estimated 99% and 95% Value-at-Risk estimates:\n\nvar_sim <- quantile(port_sim, c(0.01, 0.05))\nvar_sim\n\n       1%        5% \n-2.244965 -1.512537"
  },
  {
    "objectID": "UCL-Schoolwork/STAT0011.html#week-6-data",
    "href": "UCL-Schoolwork/STAT0011.html#week-6-data",
    "title": "STAT0011",
    "section": "Week 6 Data",
    "text": "Week 6 Data\nRequired Packages\n\nsuppressPackageStartupMessages({\n  library(VineCopula) # Copula analysis\n  library(ADGofTest) # Anderson-Darling Goodness-of-Fit test\n  library(KScorrect) # (Lilliefors-Corrected) Kolmogorov-Smirnov Goodness-of-Fit test\n  library(fGarch) # Time series analysis\n  library(tseries)\n})\n\nLoad the data:\n\nload(\"week6data.RData\")\nret1 <- dataset[,1]\nret2 <- dataset[,2]\n\nUse the Jarque–Bera test, a goodness-of-fit test of whether sample data have the skewness and kurtosis matching a normal distribution:\n\njarque.bera.test(ret1)\n\n\n    Jarque Bera Test\n\ndata:  ret1\nX-squared = 54.62, df = 2, p-value = 1.379e-12\n\njarque.bera.test(ret2)\n\n\n    Jarque Bera Test\n\ndata:  ret2\nX-squared = 888.08, df = 2, p-value < 2.2e-16\n\n\n\nModel for the Conditional Mean: AR Models - the Box-Jenkins Approach\n\nStep 1: Identification\nReturns 1\n\npar(mfrow=c(2,2))\nacf(ret1, col = \"green\", lwd = 2)\npacf(ret1, col = \"green\", lwd = 2)\nacf(ret1^2, col = \"red\", lwd = 2)\npar(mfrow=c(1,1))\n\n\n\n\nReturns 2\n\npar(mfrow = c(2, 2))\nacf(ret2, col = \"green\", lwd = 2)\npacf(ret2, col = \"green\", lwd = 2)\nacf(ret2^2, col = \"red\", lwd = 2)\npar(mfrow=c(1,1))\n\n\n\n\n\n\nStep 2: Estimation\n\nmodel1 <- garchFit(formula = ~ arma(1,0) + garch(1,1), data = ret1, trace = F, cond.dist = \"norm\")\nmodel2 <- garchFit(formula = ~ arma(3,0) + garch(1,1), data = ret2, trace = F, cond.dist = \"norm\")\n\nNote: this function uses all first three lags, not lag 3 only.\n\n\nStep 3: Model Checking\nReturns 1\n\nres1 <- residuals(model1, standardize = TRUE)\npar(mfrow = c(2, 1))\nacf(res1, col = \"green\", lwd = 2)\nacf(res1^2, col = \"red\", lwd = 2)\n\n\n\npar(mfrow = c(1, 1))\nBox.test(res1, lag = 10, type = c(\"Ljung-Box\"), fitdf = 1)\n\n\n    Box-Ljung test\n\ndata:  res1\nX-squared = 8.495, df = 9, p-value = 0.4851\n\nBox.test(res1^2, lag = 10, type = c(\"Ljung-Box\"), fitdf = 1)\n\n\n    Box-Ljung test\n\ndata:  res1^2\nX-squared = 11.657, df = 9, p-value = 0.2333\n\nmodel1@fit$ics\n\n      AIC       BIC       SIC      HQIC \n0.5658129 0.5798152 0.5658005 0.5709543 \n\nu1 <- pnorm(res1, mean = 0, sd = 1)[4:length(ret1)]\nhist(u1)\n\n\n\n\nFurther distributional checks\n\n#Kolmogorov-Smirnov test\nKStest1 <- LcKS(u1, cdf = \"punif\")\nKStest1$p.value\n\n[1] 0.418\n\n#Anderson-Darling test\nADtest1 <- ad.test(u1, null = \"punif\")\nADtest1$p.value\n\n       AD \n0.4202428 \n\n\nReturns 2\n\nres2 <- residuals(model2, standardize = TRUE)\npar(mfrow = c(2, 1))\nacf(res2, col = \"green\", lwd = 2)\nacf(res2^2, col = \"red\", lwd = 2)\n\n\n\npar(mfrow = c(1, 1))\nBox.test(res2, lag = 10, type = c(\"Ljung-Box\"), fitdf = 3)\n\n\n    Box-Ljung test\n\ndata:  res2\nX-squared = 4.4836, df = 7, p-value = 0.7227\n\nBox.test(res2^2, lag = 10, type = c(\"Ljung-Box\"), fitdf = 3)\n\n\n    Box-Ljung test\n\ndata:  res2^2\nX-squared = 7.5536, df = 7, p-value = 0.3736\n\nmodel2@fit$ics\n\n     AIC      BIC      SIC     HQIC \n2.011268 2.030871 2.011244 2.018466 \n\nu2 <- pnorm(res2, mean = 0, sd = 1)[4:length(ret2)]\nhist(u2)\n\n\n\n\nFurther distributional checks\n\n#Kolmogorov-Smirnov test\nKStest2 <- LcKS(u2, cdf = \"punif\")\nKStest2$p.value\n\n[1] 0.6628\n\n#Anderson-Darling test\nADtest2 <- ad.test(u2, null = \"punif\")\nADtest2$p.value\n\n      AD \n0.880416 \n\n\n\n\nMisspecification Example 1: Using AIC and BIC\nLet’s deliberately choosing the “wrong” AR(1) model instead of AR(3) model to show that the AIC and BIC are indeed helpful model selection criteria!\n\nmodel2b <- garchFit(formula = ~arma(1,0) + garch(1,1), data = ret2, trace = F, cond.dist = \"norm\")\nres2b <- residuals(model2b, standardize = TRUE)\n\npar(mfrow = c(1, 2))\nacf(res2b, col = \"green\", lwd = 2)\nacf(res2b^2, col = \"red\", lwd = 2)\n\n\n\npar(mfrow = c(1, 1))\n\nBox.test(res2b, lag = 10, type = c(\"Ljung-Box\"), fitdf = 1)\n\n\n    Box-Ljung test\n\ndata:  res2b\nX-squared = 784.15, df = 9, p-value < 2.2e-16\n\nBox.test(res2b^2, lag = 10, type = c(\"Ljung-Box\"), fitdf = 1)\n\n\n    Box-Ljung test\n\ndata:  res2b^2\nX-squared = 90.379, df = 9, p-value = 1.332e-15\n\nu2b <- pnorm(res2b, mean = 0, sd = 1)\nhist(u2b)\n\n\n\n#Kolmogorov-Smirnov test\nKStest2b <- LcKS(u2b, cdf = \"punif\")\nKStest2b$p.value\n\n[1] 0.3402\n\n#Anderson-Darling test\nADtest2b <- ad.test(u2b, null = \"punif\")\nADtest2b$p.value\n\n       AD \n0.4864833 \n\n\n\n\nMisspecification Example 2: Using Student-t instead of Normal\nLet’s deliberately choose the “wrong” Student-t distribution instead of a Normal distribution. We know that as the df parameter approaches infinity, the Student-t distribution approaches the Normal distribution. So for the sufficiently large value of (estimated) df parameter, we would not expect to see a big difference between the selection criteria.\n\nmodel2c <- garchFit(formula = ~arma(3,0) + garch(1,1), data = ret2, trace = F, cond.dist = \"std\")\nres2c <- residuals(model2c, standardize = TRUE)\n\npar(mfrow = c(1, 2))\nacf(res2c, col = \"green\", lwd = 2)\nacf(res2c^2, col = \"red\", lwd = 2)\n\n\n\npar(mfrow = c(1, 1))\n\nBox.test(res2c, lag = 10, type = c(\"Ljung-Box\"), fitdf = 1)\n\n\n    Box-Ljung test\n\ndata:  res2c\nX-squared = 4.7051, df = 9, p-value = 0.8592\n\nBox.test(res2c^2, lag = 10, type = c(\"Ljung-Box\"), fitdf = 1)\n\n\n    Box-Ljung test\n\ndata:  res2c^2\nX-squared = 7.8925, df = 9, p-value = 0.545\n\ncoef(model2c)\n\n          mu          ar1          ar2          ar3        omega       alpha1 \n 0.030941630  0.026238762 -0.004798470  0.561470633  0.009426896  0.094068483 \n       beta1        shape \n 0.893152245 10.000000000 \n\nu2c <- pnorm(res2c, mean = 0, sd = 1)\nhist(u2c)\n\n\n\n#Kolmogorov-Smirnov test\nKStest2c <- LcKS(u2c, cdf = \"punif\")\nKStest2c$p.value\n\n[1] 0.533\n\n#Anderson-Darling test\nADtest2c <- ad.test(u2c, null = \"punif\")\nADtest2c$p.value\n\n       AD \n0.5259121 \n\n\n\n\nCompare Three Models\n\nic <- rbind(model2@fit$ics, model2b@fit$ics, model2c@fit$ics)\nrownames(ic) <- c(\"AR(3)\", \"AR(1)\", \"Student-t\")\nic\n\n               AIC      BIC      SIC     HQIC\nAR(3)     2.011268 2.030871 2.011244 2.018466\nAR(1)     2.371386 2.385388 2.371373 2.376527\nStudent-t 2.019908 2.042311 2.019876 2.028134\n\n\nLet’s compare p-values for the KS and AD tests for uniformity:\n\ndtests <- rbind(c(KStest2$p.value, ADtest2$p.value), c(KStest2b$p.value, ADtest2b$p.value), c(KStest2c$p.value, ADtest2c$p.value))\nrownames(dtests) <- c(\"AR(3)\", \"AR(1)\", \"Student-t\")\ndtests\n\n                        AD\nAR(3)     0.6628 0.8804160\nAR(1)     0.3402 0.4864833\nStudent-t 0.5330 0.5259121\n\n\nCopula Modelling\n\nmodel <- BiCopSelect(u1, u2, familyset = NA, selectioncrit = \"AIC\", indeptest = TRUE, level = 0.05, se = TRUE)\nmodel\n\nBivariate copula: Clayton (par = 3.01, tau = 0.6) \n\n\nValue-at-Risk uisng Monte Carlo simulation:\n\nN <- 10000\nu_sim <- BiCopSim(N, family = model$family, model$par,  model$par2)\n\nHere we are assuming marginal models are N(0,1), completely ignoring our knowledge of real DGP\n\nres1_sim <- qnorm(u_sim[,1], mean = 0, sd = 1) \nres2_sim <- qnorm(u_sim[,2], mean = 0, sd = 1) \n\nHowever, “res1_sim” and “res2_sim” are i.i.d. So, the next step is to re-introduce autocorrelation and GARCH effects observed in data.\nThis section has been omitted as this will be part of your ICA group assignment…\n99% and 95% VaR:\n\nportsim <- matrix(0, nrow = N, ncol = 1)\nvarsim <- matrix(0, nrow = 1, ncol = 2)\n\nportsim <- log(1+((exp(y1simulated) - 1) + (exp(y2simulated) - 1)) * (1/2))\nvarsim <- quantile(portsim, c(0.01, 0.05))\nvarsim"
  },
  {
    "objectID": "Programming-Languages/R/Visualisation.html",
    "href": "Programming-Languages/R/Visualisation.html",
    "title": "R Visualisation",
    "section": "",
    "text": "Required Packages\n\n# install.packages('devtools')\n# devtools::install_github('bbc/bbplot')\n# if(!require(pacman))install.packages(\"pacman\")\npacman::p_load('dplyr', 'tidyr', 'gapminder',\n               'ggplot2',  'ggalt',\n               'forcats', 'R.utils', 'png', \n               'grid', 'ggpubr', 'scales',\n               'bbplot')\n\n\n\n\n# Prepare Data\nstacked_df <- gapminder %>%\n  filter(year == 2007) %>%\n  mutate(lifeExpGrouped = cut(lifeExp,\n                              breaks = c(0, 50, 65, 80, 90),\n                              labels = c(\"Under 50\", \"50-65\", \"65-80\", \"80+\"))) %>%\n  group_by(continent, lifeExpGrouped) %>%\n  summarise(continentPop = sum(as.numeric(pop)))\n\n`summarise()` has grouped output by 'continent'. You can override using the\n`.groups` argument.\n\n# Set order of stacks\nstacked_df$lifeExpGrouped = factor(stacked_df$lifeExpGrouped, levels = rev(levels(stacked_df$lifeExpGrouped)))\n\n# Create Plot\nstacked_bars <- ggplot(data = stacked_df, \n                       aes(x = continent,\n                           y = continentPop,\n                           fill = lifeExpGrouped)) +\n  geom_bar(stat = \"identity\", \n           position = \"fill\") +\n  bbc_style() +\n  scale_y_continuous(labels = scales::percent) +\n  scale_fill_viridis_d(direction = -1) +\n  geom_hline(yintercept = 0, size = 1, colour = \"#333333\") +\n  labs(title = \"How life expectancy varies\",\n       subtitle = \"% of population by life expectancy band, 2007\") +\n  theme(legend.position = \"top\", \n        legend.justification = \"left\") +\n  guides(fill = guide_legend(reverse = TRUE))\n\nstacked_bars\n\n\n\n# finalise_plot(plot_name = stacked_bars,\n#               source = \"Source: Opinion Poll\",\n#               save_filepath = \"test.png\",\n#               width_pixels = 640,\n#               height_pixels = 450)"
  }
]