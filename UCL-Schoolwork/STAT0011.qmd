---
title: STAT0011
subtitle: Decision and Risk
execute: 
  eval: false
---

# [**Z Score Calculator**](https://www.calculator.net/z-score-calculator.html)

# [**Normal Distribution Calculator**](https://www.hackmath.net/en/calculator/normal-distribution)

# **Important Distributions**

## **Continuous Distributions**

### **Normal Distribution (Gaussian Distribution)**

![](images/image-2146374466.png)

### **Uniform Distribution**

![](images/image-1370233818.png)

### **Gamma Distribution**

![](images/image-267642431.png)

# **Copula Analysis Using R**

## **Week 5 Data**

Required Packages

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
suppressPackageStartupMessages({
  library(VineCopula) # Copula analysis
  library(ADGofTest) # Anderson-Darling Goodness-of-Fit test
  library(KScorrect) # (Lilliefors-Corrected) Kolmogorov-Smirnov Goodness-of-Fit test
  library(fGarch) # Time series analysis
})
```

Load the data and have a look:

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
load("week5data.RData")
plot(dataset)
```

This does not look like a sample from a bivariate normal distribution! In fact, `ret1` follows a Student-t distribution with 10 degrees of freedom (df); `ret2` follows a Student-t distribution with 6 df. Let's assume we have this information.

Next we apply **Probability Integral Transform (PIT)** to `ret1` which has Student-t distribution with 10 df:

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
u1 <- pstd(dataset$ret1, mean = 0, sd = 1, nu = 10)
hist(u1)
```

Implement Kolmogorov-Smirnov test:

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
KStest1 <- LcKS(u1, cdf = "punif")
KStest1$p.value
```

Implement Anderson-Darling test:

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
ADtest1 <- ad.test(u1, null = "punif")
ADtest1$p.value
```

Then we apply **Probability Integral Transform (PIT)** to `ret2` which has Student-t distribution with 6 df:

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
u2 <- pstd(dataset$ret2, mean = 0, sd = 1, nu = 6)
hist(u2)
```

Implement Kolmogorov-Smirnov test:

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
KStest2 <- LcKS(u2, cdf = "punif")
KStest2$p.value
```

Implement Anderson-Darling test:

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
ADtest2 <- ad.test(u1, null = "punif")
ADtest2$p.value
```

We **pass** the **test for uniformity** for both transformed log-returns, so we can proceed to copula modelling.

Using `BiCopSelect` function fit various copulas to the dataset and select the copula that provides the best fit based on the **AIC** criterion:

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
plot(cbind(u1, u2))
model <- BiCopSelect(u1, u2, familyset = NA, selectioncrit = "AIC", indeptest = TRUE, level = 0.05)
model
```

The model of best fit is **Clayton with the estimated parameter theta = 1.94**.

Next we estimate the Value-at-Risk using the **Monte Carlo simulation** approach based on copula theory:

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
N <- 1800
set.seed(0123)
u_sim <- BiCopSim(N, family = 3, model$par) # family = 3 means Clayton
```

Next we apply the component-wise **Inverse Probability Integral Transform (IPIT)** to both `ret1` and `ret2`.

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
ret1_sim <- qstd(u_sim[,1], mean = 0, sd = 1, nu = 10)
ret2_sim <- qstd(u_sim[,2], mean = 0, sd = 1, nu = 6)
```

Note that our selected copula model is capable of generating values of log-returns with the observed dependence structure:

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
par(mfrow=c(2,1))
plot(dataset, ylab = "ret2", xlab = "ret1", col = "blue", main = "Original log-returns")
plot(data.frame(ret1_sim, ret2_sim), ylab = "ret2_sim", xlab = "ret1_sim",
     col = "blue", main = "Simulated log-returns")
par(mfrow=c(1,1))
```

Next, compute portfolio log-returns:

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
port_sim <- matrix(0, nrow = N, ncol = 1)
VaR_sim <- matrix(0, nrow = 1, ncol = 2)

port_sim <- log(1 + ((exp(ret1_sim) - 1) + (exp(ret2_sim) - 1)) * (1 / 2)) # Need to transform individual log-returns back to individual net returns, and compute portfolio net returns, finally compute portfolio log-returns.
```

Return the estimated 99% and 95% Value-at-Risk estimates:

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
var_sim <- quantile(port_sim, c(0.01, 0.05))
var_sim
```

# **Time Series Analysis Using R**

## **Week 6 Data**

Required Packages

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
suppressPackageStartupMessages({
  library(VineCopula) # Copula analysis
  library(ADGofTest) # Anderson-Darling Goodness-of-Fit test
  library(KScorrect) # (Lilliefors-Corrected) Kolmogorov-Smirnov Goodness-of-Fit test
  library(fGarch) # Time series analysis
  library(tseries)
})
```

Load the data:

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
load("week6data.RData")
ret1 <- dataset[,1]
ret2 <- dataset[,2]
```

Use the **Jarque--Bera test**, a [goodness-of-fit](https://en.wikipedia.org/wiki/Goodness-of-fit "Goodness-of-fit") test of whether sample data have the [skewness](https://en.wikipedia.org/wiki/Skewness "Skewness") and [kurtosis](https://en.wikipedia.org/wiki/Kurtosis "Kurtosis") matching a [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution "Normal distribution"):

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
jarque.bera.test(ret1)
jarque.bera.test(ret2)
```

### **Model for the Conditional Mean: AR Models - the Box-Jenkins Approach**

#### **Step 1: Identification**

**Returns 1**

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
plot(ret1, type = "l")

par(mfrow = c(2, 2))
acf(ret1, col = "green", lwd = 2)
pacf(ret1, col = "green", lwd = 2)
acf(ret1^2, col = "red", lwd = 2) # for GARCH effects
par(mfrow = c(1, 1))
```

We can determine the lags of AR model via the **Partial ACF plot**.

We can use the ACF plot for the squared returns to see if we need to introduce **GARCH effects**, we need to try `garch(1, 1)` up until `garch(3, 3)`.

**Returns 2**

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
plot(ret2, type = "l")

par(mfrow = c(2, 2))
acf(ret2, col = "green", lwd = 2)
pacf(ret2, col = "green", lwd = 2)
acf(ret2^2, col = "red", lwd = 2) # for GARCH effects
par(mfrow = c(1, 1))
```

We can determine the lags of AR model via the **Partial ACF plot**.

We can use the ACF plot for the squared returns to see if we need to introduce **GARCH effects**, we need to try `garch(1, 1)` up until `garch(3, 3)`.

#### **Step 2: Estimation**

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
model1 <- garchFit(formula = ~ arma(1, 0) + garch(1,1), data = ret1, trace = F, cond.dist = "norm")
model2 <- garchFit(formula = ~ arma(3, 0) + garch(1,1), data = ret2, trace = F, cond.dist = "norm")
```

Note: this function uses **all first three lags**, not lag 3 only.

We can plot the histogram of `ret1` and `ret2` to decide the right distribution for argument `cond.dist =` . We also need to use Kolmogorov-Smirnov test and Anderson-Darling test (see later).

#### **Step 3: Model Checking**

**Returns 1**

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
res1 <- residuals(model1, standardize = TRUE)

par(mfrow = c(2, 1))
acf(res1, col = "green", lwd = 2)
acf(res1^2, col = "red", lwd = 2)
par(mfrow = c(1, 1))

# The Box-Ljung test is to test if there exists autocorrelation, the null is no autocorrelation. We use fitdf = 1 because we used AR(1) model earlier.
Box.test(res1, lag = 10, type = c("Ljung-Box"), fitdf = 1)
Box.test(res1^2, lag = 10, type = c("Ljung-Box"), fitdf = 1)

model1@fit$ics

u1 <- pnorm(res1, mean = 0, sd = 1)[4:length(ret1)]
hist(u1)
```

**Further distributional checks:**

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
#Kolmogorov-Smirnov test
KStest1 <- LcKS(u1, cdf = "punif")
KStest1$p.value

#Anderson-Darling test
ADtest1 <- ad.test(u1, null = "punif")
ADtest1$p.value
```

**Returns 2**

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
res2 <- residuals(model2, standardize = TRUE)

par(mfrow = c(2, 1))
acf(res2, col = "green", lwd = 2)
acf(res2^2, col = "red", lwd = 2)
par(mfrow = c(1, 1))

# The Box-Ljung test is to test if there exists autocorrelation, the null is no autocorrelation. We use fitdf = 3 because we used AR(3) model earlier.
Box.test(res2, lag = 10, type = c("Ljung-Box"), fitdf = 3)
Box.test(res2^2, lag = 10, type = c("Ljung-Box"), fitdf = 3)

model2@fit$ics

u2 <- pnorm(res2, mean = 0, sd = 1)[4:length(ret2)]
hist(u2)
```

**Further distributional checks:**

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
#Kolmogorov-Smirnov test
KStest2 <- LcKS(u2, cdf = "punif")
KStest2$p.value

#Anderson-Darling test
ADtest2 <- ad.test(u2, null = "punif")
ADtest2$p.value
```

##### Misspecification Example 1: Using AIC and BIC

Let's deliberately choosing the "wrong" AR(1) model instead of AR(3) model to show that the AIC and BIC are indeed helpful model selection criteria!

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
model2b <- garchFit(formula = ~arma(1,0) + garch(1,1), data = ret2, trace = F, cond.dist = "norm")
res2b <- residuals(model2b, standardize = TRUE)

par(mfrow = c(1, 2))
acf(res2b, col = "green", lwd = 2)
acf(res2b^2, col = "red", lwd = 2)
par(mfrow = c(1, 1))

Box.test(res2b, lag = 10, type = c("Ljung-Box"), fitdf = 1)
Box.test(res2b^2, lag = 10, type = c("Ljung-Box"), fitdf = 1)

u2b <- pnorm(res2b, mean = 0, sd = 1)
hist(u2b)

#Kolmogorov-Smirnov test
KStest2b <- LcKS(u2b, cdf = "punif")
KStest2b$p.value

#Anderson-Darling test
ADtest2b <- ad.test(u2b, null = "punif")
ADtest2b$p.value
```

##### Misspecification Example 2: Using Student-t instead of Normal

Let's deliberately choose the "wrong" Student-t distribution instead of a Normal distribution. We know that as [the df parameter approaches infinity, the Student-t distribution approaches the Normal distribution]{.underline}. So for the sufficiently large value of (estimated) df parameter, we would not expect to see a big difference between the selection criteria.

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
model2c <- garchFit(formula = ~arma(3,0) + garch(1,1), data = ret2, trace = F, cond.dist = "std")
res2c <- residuals(model2c, standardize = TRUE)

par(mfrow = c(1, 2))
acf(res2c, col = "green", lwd = 2)
acf(res2c^2, col = "red", lwd = 2)
par(mfrow = c(1, 1))

Box.test(res2c, lag = 10, type = c("Ljung-Box"), fitdf = 1)
Box.test(res2c^2, lag = 10, type = c("Ljung-Box"), fitdf = 1)
coef(model2c)

u2c <- pnorm(res2c, mean = 0, sd = 1)
hist(u2c)

#Kolmogorov-Smirnov test
KStest2c <- LcKS(u2c, cdf = "punif")
KStest2c$p.value

#Anderson-Darling test
ADtest2c <- ad.test(u2c, null = "punif")
ADtest2c$p.value
```

#### **Compare Three Models**

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
ic <- rbind(model2@fit$ics, model2b@fit$ics, model2c@fit$ics)
rownames(ic) <- c("AR(3)", "AR(1)", "Student-t")
ic
```

Let's compare p-values for the KS and AD tests for uniformity:

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
dtests <- rbind(c(KStest2$p.value, ADtest2$p.value), c(KStest2b$p.value, ADtest2b$p.value), c(KStest2c$p.value, ADtest2c$p.value))
rownames(dtests) <- c("AR(3)", "AR(1)", "Student-t")
dtests
```

#### **Copula Modelling**

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
model <- BiCopSelect(u1, u2, familyset = NA, selectioncrit = "AIC", indeptest = TRUE, level = 0.05, se = TRUE)
model
```

Value-at-Risk uisng Monte Carlo simulation:

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
N <- 10000
u_sim <- BiCopSim(N, family = model$family, model$par,  model$par2)
```

Here we are assuming marginal models are N(0,1), completely ignoring our knowledge of real DGP

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
res1_sim <- qnorm(u_sim[,1], mean = 0, sd = 1) 
res2_sim <- qnorm(u_sim[,2], mean = 0, sd = 1) 
```

However, "res1_sim" and "res2_sim" are i.i.d. So, the next step is to **re-introduce autocorrelation and GARCH effects** observed in data.

This section has been omitted as this will be part of your ICA group assignment...

99% and 95% VaR:

```{r, eval=FALSE, echo=TRUE, warning=FALSE}
portsim <- matrix(0, nrow = N, ncol = 1)
varsim <- matrix(0, nrow = 1, ncol = 2)

portsim <- log(1+((exp(y1simulated) - 1) + (exp(y2simulated) - 1)) * (1/2))
varsim <- quantile(portsim, c(0.01, 0.05))
varsim
```
