---
title: STAT0023
subtitle: ACE your ICA!
execute: 
  eval: false
---

# STAT0023 Review Notes {#welcome}

**Required Packages**

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
suppressPackageStartupMessages({
  library(Hmisc)
  library(RColorBrewer)
  library(ggplot2)
  library(maps)
  library(robustbase)
  library(mgcv) # for gam()
})
```

## **Week 1**

### **R Basics**

#### **NA**

![NA (Not Available): missing values; NaN (Not a Number): illegal operations](images/NA1.png){alt="NA (Not Available): missing values; NaN (Not a Number): illegal operations" fig-align="left" width="800"}

![](images/NA2.png){fig-align="left" width="800"}

![](images/NA3.png){fig-align="left" width="800"}

#### **NaN**

![](images/NaN1.png){fig-align="left" width="800"}

### **Exploratory Analysis**

#### **Galapagos Islands Data (species.data)**

```{r}
species.data <- read.table("galapagos.dat", header = TRUE)
```

What is the name of the (smallest) / (second largest) Galapagos island in the dataset?

```{r}
## the smallest
rownames(species.data)[which.min(species.data$Area)]
## the second largest
rownames(species.data)[order(species.data$Area, decreasing = TRUE)[2]]
```

How many plant species are there in total on the second largest Galapagos island in the dataset?

```{r}
species.data$Species[order(species.data$Area, decreasing = TRUE)[2]]
```

What is the name of the island with a value of 25 for the Elevation variable?

```{r}
rownames(species.data)[species.data$Elevation == 25]
```

How many islands have fewer than 25 species in total?

```{r}
sum(species.data$Species < 25)
```

What is the estimated slope in a linear regression of Area upon Scruz?

```{r}
Model <- lm(Area ~ Scruz, data = species.data)
summary(Model)
print(Model$coefficients, digits = 8)
```

#### **Iris Data**

```{r}
iris <- iris
```

What is the mean of the petal widths for all of the flowers in the dataset?

```{r}
mean(iris$Petal.Width)
```

What is the mean of the petal widths for all of the "setosa" flowers in the dataset?

```{r}
mean(iris$Petal.Width[iris$Species == "setosa"])
## or
tapply(iris$Petal.Width, INDEX = iris$Species, FUN = mean)
```

Considering the petal lengths of the flowers, which of the three species has the largest standard deviation?

```{r}
tapply(iris$Petal.Length, INDEX = iris$Species, FUN = sd)
```

What is the 40th percentile of the petal lengths for all of the flowers in the dataset?

```{r}
print(quantile(iris$Petal.Length, probs = 0.4), digits = 8)
```

Considering the petal lengths of the flowers, which of the three species has the largest 90th percentile?

```{r}
tapply(iris$Petal.Length, INDEX = iris$Species, FUN = quantile, probs = 0.9)
```

Carry out a two-tailed *t* test, assuming equal variances, for a difference between the population mean petal widths for "versicolor" and "virginica" flowers.

```{r}
result <- t.test(iris$Petal.Width[iris$Species == "versicolor"],
                 iris$Petal.Width[iris$Species == "virginica"],
                 var.equal = TRUE)
print(result, digits = 8)
```

Carry out an *F* test of the null hypothesis that the population variance of sepal lengths is the same for flowers of "setosa" and "virginica" species.

```{r}
result <- var.test(iris$Sepal.Length[iris$Species == "setosa"],
                   iris$Sepal.Length[iris$Species == "virginica"])
print(result, digits = 8)
```

To three decimal places, what is the lower limit of a 90% confidence interval for the difference between the population mean petal widths for "versicolor" and "virginica" flowers?

```{r}
result <- t.test(iris$Petal.Width[iris$Species == "versicolor"],
                 iris$Petal.Width[iris$Species == "virginica"],
                 var.equal = TRUE,
                 conf.level = 0.90)
print(result, digits = 8)
```

### **Graphics**

```{r}
x <- seq(-2, 2, 0.05)
x2 <- x^2
x3 <- x^3
plot(x, x2)
lines(x, x3, lwd = 2, col = "red")
```

#### **ChickWeight Data**

```{r}
ChickWeight <- ChickWeight
```

#### **CO2 Data**

```{r}
CO2 <- CO2
```

## **Week 2**

### **Regression Models: Assumptions and Interpretation**

#### **Deal With Standard Errors**

![](images/SD1.png){fig-align="left" width="800"}

![](images/SD2.png){fig-align="left" width="800"}

#### **Standard Assumptions of the Linear Regression Model**

![](images/Assumptions1.png){fig-align="left" width="800"}

![](images/Assumptions2.png){fig-align="left" width="800"}

#### **Iris Data**

```{r}
iris <- iris
```

##### **Model 0 (No Intercept)**

```{r}
Model <- lm(Petal.Length ~ Species - 1, data = iris)
summary(Model)
```

The coefficient estimates in the above model (no intercept) are just [**the sample means**]{.underline}.

##### **Model 1 (Intercept Included)**

```{r}
Model <- lm(Petal.Length ~ Species, data = iris)
summary(Model)
```

The intercept in the above model (intercept included) is the [**Setosa sample mean**]{.underline}. The coefficient estimates are the [**differences**]{.underline} between the [**respective means**]{.underline} and the [**Setosa mean**]{.underline}.

##### **Model 2 ("Sum to Zero" Constraint)**

```{r}
Model <- lm(Petal.Length ~ Species, data = iris, contrasts = list(Species = "contr.sum"))
summary(Model)
```

Species 1: Setosa

Species 2: Versicolor

Species 3: Virginica

The intercept in the above model ("sum to zero" constraint) is the [**overall mean**]{.underline} petal length for all the iris flowers. The coefficients represent the [**differences**]{.underline} between the [**respective means**]{.underline} and the [**overall mean**]{.underline}.

##### **Model 3**

```{r}
Model <- lm(Petal.Length ~ Species + Sepal.Length, data = iris)
summary(Model)
```

The intercept in the above model is [**the expected petal length for Setosa when the sepal length is zero**]{.underline}.

##### **Model 4 (Interaction Included)**

```{r}
Model <- lm(Petal.Length ~ Species + Sepal.Length + Species:Sepal.Length, data = iris)
summary(Model)
```

The main effect coefficients for Versicolor and Virginica represent [**differences relative to Setosa**]{.underline}. Two additional coefficients for the interactions represent [**differences**]{.underline} in the [**slopes of the Petal.Length:Sepal.Length relationship**]{.underline} for these two species, [**relative to the slope for Setosa flowers**]{.underline}.

[**Note**]{.underline}: Interactions have [**nothing to do with correlations**]{.underline} between covariates: rather, they relate to the way in which the relationship between one covariate and the response varies with another covariate.

#### **UStemps Data**

```{r}
load("UStemps.rda")
ustemp$longitude <- -ustemp$longitude
```

##### **Model 1 (Base Model)**

```{r}
Model1 <- lm(min.temp ~ latitude + longitude, data = ustemp)
summary(Model1)
```

##### **Model 2 (Quadratic Model)**

```{r}
Model2 <- lm(min.temp ~ latitude + longitude + I(latitude^2) + I(longitude^2) + latitude:longitude, data = ustemp)
summary(Model2)
```

###### **Model 2 (Quadratic Model) Diagnostics Plots**

```{r}
par(mfrow = c(2,2),
    mar = c(3,3,2,2),
    mgp = c(2,0.75,0))
plot(Model2, which = 1:4)
```

Cook's Distance plot shows three observations that may be influential, they are:

```{r}
ustemp[c(5, 13, 52), ]
## 5: Los Angeles, CA
## 13: Miami, FL
## 52: Seattle, WA
```

##### **Model 2Rob (Robust Fit of the Quadratic Model)**

```{r}
Model2Rob <- lmrob(min.temp ~ latitude + longitude + I(latitude^2) + I(longitude^2) + latitude:longitude, data = ustemp)
summary(Model2Rob)
```

**Note**: the purpose of robust estimation is to try and [**include all of the observations**]{.underline}, but to [**limit the extent**]{.underline} to which any individual observation can influence the fit.

### **Get your Eyes in: Residual Plots**

#### **Failures of Assumptions**

The main things that can result in [**incorrect standard errors**]{.underline} are [**non-constant variance**]{.underline} and [**dependence between residuals**]{.underline}

-   **Systematic structure in mean residuals**

    "Residuals vs Fitted Values" plot, or "Residuals vs Covariates" plot.

    Such structure indicates that the modelled representation of the regression function is inadequate. Whether this matters is context-specific: you may decide, for example, that the residuals are all so small that your model is already predicting well enough.

-   **Non-constant variance**

    "Residuals vs Fitted Values" plot may exhibit a "funnel" shape (i.e. the residual variance seems higher at one end than the other).

    "Scale-Location" plot: the absolute residuals tend to be larger at one end than the other.

    In this case, the least-squares estimates are not fully efficient (i.e. you're not making the best use of your data), and the [**reported standard errors will be incorrect**]{.underline} --- as will the [**results of any hypothesis tests and confidence interval calculations**]{.underline}.

-   **Non-normal residuals**

    "Normal Q-Q plot": the residuals don't fall roughly on a straight line.

    This is not critical in large samples, the exception is where you want to calculate prediction intervals for future observations: these will only be accurate if the future observations do indeed have an approximate normal distribution.

-   **Lack of independence**

    Common situations in which it might be a problem are when data are collected at successive time points, or at a collection of spatial locations. Although none of the 'standard' plots is designed to check for this, lack of independence can lead to apparent structure in some of the residual plots. For example, if observations are collected sequentially in time and successive residuals are highly correlated, this can give the appearance of curvature in the "[**Residuals vs Fitted Values**]{.underline}" plot: the curvature is not due to a nonlinear relationship between response and covariates, but simply due to the fact that [**neighbouring residuals are similar to each other because they are correlated**]{.underline}.

#### **Influential Observations**

Rule-of-thumb: observation influential if **Cook's distance** **exceeds** $\frac{8}{n-2k}$, where `k` is the number of coefficients estimated.

#### **Fix the Problem**

-   [**Transform**]{.underline} the response variable and / or covariate (but only if the resulting model makes scientific sense). Sometimes, a relationship can be made more linear by [**taking logs or square roots of one or more quantities**]{.underline}; transforming the response variable can also help to make the residual variance more constant, and to make the assumption of normality more reasonable. Don't take logs (or square roots) of quantities that could be negative, though!

-   [**Add additional terms to the model**]{.underline}. For the temperature data for example, you might consider extending the quadratic model to include third-degree terms in latitude and longitude; or additional covariates such as altitude if these were available.

-   If the diagnostics suggest that the [**residual variance is related to the fitted values**]{.underline} (e.g. the 'residuals versus fitted values' plot has a funnel shape) and there is good reason to suspect that the responses follow [**non-normal distributions**]{.underline} (e.g. because they are counts, so that Poisson distributions might be more appropriate) then a more general class of models may be appropriate --- such as generalized linear models (GLMs).

## **Week 3**

### **Basic Programming Techniques**

#### **Logical Expressions**

![](images/Logical1.png){fig-align="left" width="800"}

#### **Write Some Expressions**

Return TRUE if there is at least one strictly negative value in a vector x, and FALSE otherwise.

```{r}
any(x<0)
```

Extract the elements of a vector x that are both (i) strictly positive (ii) divisible by 10.

```{r}
x[x>0&x%%10==0]
```

Extract the rows of a matrix or data frame y for which the corresponding elements of a vector x are non-negative, where x is a vector with length equal to the number of rows of y.

```{r}
y[x>=0,]
```

Extract the elements of a vector y for which the corresponding elements of x are strictly negative.

```{r}
y[x<0]
```

#### **`trapezium()` Function**

Create a function to approximate an integral using the trapezium rule with equally spaced x values.

```{r}
trapezium <- function(v, a, b) {
  n <- length(v) - 1
  h <- (b - a) / n
  intv <- (h / 2) * (sum(v) + sum(v[2 : n]))
  intv
}
```

Use the above function to evaluate the integral $\int_{1}^{5}arctan(x)dx$ with 39 evaluation points (i.e. so that the input vector v has 39 elements).

```{r}
trapezium(atan(seq(1, 5,length.out = 39)), 1, 5)
```

## **Week 4**

### **Function Minimisation**

#### **Newton-Raphson Algorithm**

Create a `QuarticMin()` function to minimise function $h(x)=4x^2+5x+3$, with starting value $x_0=-3$.

```{r}
QuarticMin <- function(x0, tol = 1e-6, MaxIter = 100, Verbose = TRUE) {
  x <- x0
  Iter <- 0
  dh.dx <- (8 * x) + 5 # First derivative
  RelChange <- Inf
  while (abs(dh.dx) > tol & abs(RelChange) > tol & Iter < MaxIter) {
    if (Verbose) {
      cat(paste("Iteration",Iter,":   Estimate =",signif(x,5),
                "    Gradient = ",signif(dh.dx,5),"\n"))
    }
    x.old <- x
    d2h.dx2 <- 8 # Second derivative
    x <- x - (dh.dx / d2h.dx2)
    dh.dx <- (8 * x) + 5 # First derivative
    RelChange <- (x-x.old) / x.old
    Iter <- Iter + 1
  }
  if (Verbose) cat("---\n")
  hmin <- (4 * x^2) + (5 * x) + 3 # Original
  list(x=x, hx=hmin, gradient=dh.dx, N.iter=Iter)
}

QuarticMin(-3)
```

[**Note**]{.underline}: In Iteration `n`, Estimate = $x_{n}$.

#### **`tploglik()` Function**

Create a `tploglik()` function to returns the negative log likelihood of a truncated Poisson distribution, up to a constant.

```{r}
tploglik <- function(theta, y) {
  n <- length(y)
  ybar <- mean(y)
  n * (theta - (ybar * log(theta)) + log(1 - exp(-theta)))
}
```

Truncated Poisson Distribution Data

```{r}
TPdata <- rep(1:14, c(97, 164, 242, 182, 154, 83, 44, 19, 9, 7, 2, 0, 0, 1))
```

Use `nlm()` and `tploglik()` functions to find the maximum likelihood estimate of $\theta$ together with its estimated standard error. Use the sample mean of the original 1004 values as the starting point for `nlm()`.

```{r}
TP.fitted <- nlm(tploglik, mean(TPdata), y = TPdata, hessian = TRUE)
```

```{r}
## The MLE of theta
TP.fitted$estimate
## Estimated Standard Error of the MLE
1 / sqrt(TP.fitted$hessian)
```

#### **Non-linear Least Squares estimates**

Data

```{r}
NLSdata <- read.table(file = "nls2.dat", header = TRUE)
```

Fit a linear model **regressing `log(Y)` on `x`**

```{r}
Model_Log <- lm(log(Y) ~ x, data = NLSdata)
summary(Model_Log)
```

`sumsqerr()` function to compute sum of squared errors

```{r}
sumsqerr <- function(theta, x, Y) {
  beta0 <- theta[1]
  beta1 <- theta[2]
  mu <- beta0 * exp(beta1 * x)
  sum((Y - mu)^2)
}
```

For your nonlinear model, what is the sum of squared errors for the parameter values $\beta_0=1.4$, $\beta_1=-0.1$?

```{r}
sumsqerr(c(1.4, -0.1), x = NLSdata$x, Y = NLSdata$Y)
```

Try fitting your nonlinear model using `nlm()`, with starting values $\beta_0=-0.8$ and $\beta_1=-0.6$. Compare the minimised sum of squares with the value that you obtained in the workshop starting from $\beta_0=1.3$ and $\beta_1=-0.3$. Has `nlm()` located (approximately) the same minimum this time?

```{r}
NLS.fit1 <- nlm(sumsqerr, c(-0.8, -0.6), x = NLSdata$x, Y = NLSdata$Y, hessian = TRUE)
NLS.fit2 <- nlm(sumsqerr, c(1.3, -0.3), x = NLSdata$x, Y = NLSdata$Y, hessian = TRUE)
```

Try fitting the model using `nlm()`, with starting values $\beta_0=0.2$ and $\beta_1=0.4$. At the estimated minimum, what is the (2, 2) element of the Hessian matrix of the sum of squares?

```{r}
NLS.fit3 <- nlm(sumsqerr, c(0.2, 0.4), x = NLSdata$x, Y = NLSdata$Y, hessian = TRUE)
NLS.fit3$hessian[2, 2]
```

## **Week 6**

### **Generalized Linear Models (GLMs)**

#### **Table of means, variance functions and dispersion parameters for some common distributions in the exponential family**

![](images/image-117798365.png)

#### **Galapagos Islands Data (Poisson GLM)**

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
galapagos.data <- read.table("galapagos.dat")
```

##### **Model 1 (GLM with log link)**

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
galapagos.glm1 <- glm(Endemics ~ log(Area), 
                      family = poisson(link = "log"),
                      data = galapagos.data)
summary(galapagos.glm1)
```

In this case, the model says that Y\[i\] (number of endemics for the ith island) comes from a [Poisson distribution with mean mu\[i\]]{.underline}, where

log(mu\[i\]) = beta0 + (beta1 \* x\[i\])

and [x\[i\] is the log of the area of the ith island]{.underline}. The coefficients in the output are estimates of beta0 and beta1 respectively.

Notice also the "**Null deviance**" (this is the deviance - or **lack of fit** - for a model **containing just a constant term**, analogous to the total sum of squares in a linear regression model), and the "**Residual deviance**" (the lack of fit for the model you've just fitted, analagous to the residual sum of squares). The residual deviance is **much lower** than the null deviance, suggesting that the model has **"explained" a lot of the variation** - just as you would say that a linear regression model explains a lot of the variation if the residual sum of squares is much lower than the total sum of squares.

Finally, notice the AIC (Akaike Information Criterion). This is another measure of "model fit", a really good model will have a high log-likelihood with few parameters. **The lower the AIC, the better the model**.

###### **Residual plots of Model 1 (GLM with log link)**

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
par(mfrow = c(2,2), lwd = 2, mar = c(3,3,2,2), mgp = c(2,0.75,0))
plot(galapagos.glm1, which = 1:4)
```

The Cook's distance plot shows that there are some islands which do potentially have a **big influence on the results**; and there are also several residuals outside the range **-2 to 2** (look at the "residuals vs fitted" plot).

This suggests that the endemic [species counts are more variable than we would expect from a Poisson distribution]{.underline}. This phenomenon is called **overdispersion**. One possible explanation is that [there are other factors in addition to island area that influence the number of endemic species]{.underline}. This is quite likely, in fact.

The next question is: what might those other factors be? And do we have the data?! The variables in the include "`Nearest`" (distance to the nearest other island in km) and "`Adjacent`" (the area of the nearest other island in km\^2). It seems reasonable to imagine that if the nearest other island is very close by, then it is easy for species to move between islands and therefore that the number of endemics is likely to be lower. Let's see whether there's any evidence for that.

To help understand the model that you've just fitted, let's pretend that all the assumptions are satisfied so that it makes sense to plot the fitted relationship - together with some confidence intervals, perhaps. Here goes:

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
x.grid <- seq(min(galapagos.data$Area),
              max(galapagos.data$Area), by = 0.1)
Endemics.pred <- predict(galapagos.glm1,
                         newdata = data.frame(Area = x.grid),
                         se.fit = TRUE)

z <- qnorm(0.975, mean = 0, sd = 1, lower.tail = TRUE)
UL95 <- Endemics.pred$fit + (z*Endemics.pred$se.fit)
LL95 <- Endemics.pred$fit - (z*Endemics.pred$se.fit)

par(mfrow = c(1,1))
plot(galapagos.data$Area, galapagos.data$Endemics, col = "lightblue",
     pch = 15,log = "x",xlab = expression("Island area (km"^2*")"),ylab = "# endemics",
     main = "Galapagos islands: numbers of endemic species")
lines(x.grid,exp(Endemics.pred$fit),col="blue")
lines(x.grid,exp(UL95),lty=2,col="blue")
lines(x.grid,exp(LL95),lty=2,col="blue")
```

##### **Model 2 (Model 1 + `Nearest`)**

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
galapagos.glm2 <- update(galapagos.glm1, . ~ . + Nearest)
summary(galapagos.glm2)
```

According to the z test, the `Nearest` variable is not quite significant at 5% level.

Do a chi-squared test to compare `Model 1` and `Model 2` :

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
anova(galapagos.glm1, galapagos.glm2, test="Chi")
```

The chi-squared test result suggests that we **can reject the null hypothesis** at the 5% level! Actually, the [anova() results are usually more accurate]{.underline} than the z tests in the summary() output - however, in this particular instance everything seems too close to call.

It is also possible that the number of endemics is influenced by the size of the neighbouring island - but this probably won't make much difference if that island is 200km away! This suggests adding the variable `Adjacent` to the model, as well as an **interaction between** `Adjacent` **and** `Nearest`.

##### **Model 3 (Model 2 + `Adjacent` + `Adjacent:Nearest`)**

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
galapagos.glm3 <- update(galapagos.glm2, . ~ . + Adjacent + Adjacent:Nearest)
summary(galapagos.glm3)
```

Do another chi-squared test to compare `Model 2` and `Model 3` :

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
anova(galapagos.glm2, galapagos.glm3, test = "Chi")
```

###### **Residual plots of Model 3 (Model 2 + `Adjacent` + `Adjacent:Nearest`)**

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
par(mfrow = c(2,2), lwd = 2, mar = c(3,3,2,2), mgp = c(2,0.75,0))
plot(galapagos.glm3, which = 1:4)
```

Well ... there are still several residuals outside the range (-2,2). As a check on this, let's calculate the variance of the Pearson residuals - which, for a **Poisson model**, are defined in such a way that if the model is correct their **mean and variance should be roughly 0 and 1 respectively** (see workshop notes). When calculating their variance, we need to account for the number of parameters estimated in the model - so we can't just use the var() command directly.

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
sum(resid(galapagos.glm3, type = "pearson")^2 ) / galapagos.glm3$df.residual
```

That's not even close to 1! Conclusion: we haven't succeeded in explaining the excess variation. This means that [all of the existing test results, confidence intervals and so forth will be incorrect]{.underline} because they are calculated under the assumption that the variance is equal to the mean. There isn't much that we can do to improve the model further, but there is a trick that we can use to at least ensure that the test results are correct. This is to introduce a "dummy" **dispersion parameter** into the Poisson model, which just [allows all of the standard errors to be increased to acknowledge the additional variability in the data]{.underline}.

R allows this via the use of a "**quasipoisson**" family of distributions (there is also a "**quasibinomial**" family, that can be used for **overdispersed** binomial data). Be aware that there is no such thing in reality as a "quasiPoisson" or "quasibinomial" distribution: they are provided here solely because it is quite common to encounter **overdispersion** in applications, and the inclusion of a "dummy" dispersion parameter provides an easy (and legitimate!) way to ensure that standard errors etc. are correct. Here we go:

##### **Model 4 (Model 3 with family changed to quasipoission)**

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
galapagos.glm4 <- update(galapagos.glm3, family = quasipoisson(link = "log"))
summary(galapagos.glm4)
```

Notice that the **AIC** is reported as `NA`. This is because the AIC requires a log-likelihood, and the log-likelihood is computed from the probability density or probability mass function of the fitted model, and there is no such thing as "the probability mass function of a quasiPoisson distribution". You might reasonably ask "how can you estimate the parameters by maximum likelihood then?" The answer - at least, for the purposes of this course - is: the parameters are estimated as though the data follow a [Poisson distribution]{.underline}, and then [the "dummy" dispersion parameter is introduced afterwards]{.underline}.

We can't use the anova() command to compare the quasipoisson model with any of the earlier models: it can only be used to compare nested models [within the same distributional "family"]{.underline}. However, if you call anova() with just a single model then it carries out a sequence of tests as though you **built up the model in stages, at each stage adding one extra term**. Like this (notice the use of `test="F"`, because we now have a dispersion parameter).

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
anova(galapagos.glm4, test = "F")
```

#### **Beetle Mortality Data (Binomial GLM)**

##### **Three commonly used link functions and their inverse functions for the binomial distribution**

![](images/image-1757126326.png)

![](images/image-599157655.png)

##### Define the data:

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
logdose <- c(1.6907, 1.7242, 1.7552, 1.7842, 1.8113, 1.8369, 1.8610, 1.8839)
totals <- c(59, 60, 62, 56, 63, 59, 62, 60)
killed <- c(6, 13, 18, 28, 52, 53, 61, 60)
proportions <- killed / totals
```

##### **Model 1 (Binomial GLM with logit link)**

Notice the "weights" argument: When using glm() with binomial data, this is needed to specify the values of "n" for each observation (if you think about it, an observed proportion from a group containing n=1000 insects will be more accurate than a proportion from a group containing just n=10 insects.

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
beetle.glm <- glm(proportions ~ logdose, weights = totals, 
                  family = binomial(link = "logit"))
summary(beetle.glm)
```

###### **Diagnostics of Model 1**

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
par(mfrow = c(2,2), lwd = 2, mar = c(3,3,2,2), mgp = c(2,0.75,0))
plot(beetle.glm, which = 1:4)
```

##### **Model 2 (Binomial GLM with probit link)**

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
beetle.glm.probit <- glm(proportions ~ logdose, weights = totals,
                         family=binomial(link="probit") )
summary(beetle.glm.probit)
```

###### **Diagnostics of Model 2**

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
par(mfrow = c(2,2), lwd = 2, mar = c(3,3,2,2), mgp = c(2,0.75,0))
plot(beetle.glm.probit, which = 1:4)
```

##### **Model 3 (Binomial GLM with complementary log-log link)**

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
beetle.glm.cloglog <- glm(proportions ~ logdose, weights = totals,
                         family=binomial(link="cloglog") )
summary(beetle.glm.cloglog)
```

###### **Diagnostics of Model 3**

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
par(mfrow = c(2,2), lwd = 2, mar = c(3,3,2,2), mgp = c(2,0.75,0))
plot(beetle.glm.cloglog, which = 1:4)
```

Seems that `Model 3` is better than the other two.

Produce a plot of the fitted dose-response curve with some confidence intervals using `Model 3`:

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
x.grid <- seq(1.6, 1.9, 0.01)
Beetle.pred <- predict(beetle.glm.cloglog, newdata = data.frame(logdose = x.grid), se.fit = TRUE)

z <- qnorm(0.975, mean = 0,sd = 1,lower.tail = TRUE)
UL95 <- Beetle.pred$fit + (z * Beetle.pred$se.fit)
LL95 <- Beetle.pred$fit - (z * Beetle.pred$se.fit)

par(mfrow=c(1,1))
plot(logdose, proportions,
     xlab = expression(log[10]*" CS"[2]*" concentration (mg\ l"^{-1}*")"),
     ylab = "Proportion",main = "Proportion of insects killed",
     col = "salmon",pch = 15)

lines(x.grid, 1 - exp(-exp(Beetle.pred$fit)), col="red")
lines(x.grid, 1 - exp(-exp(UL95)), lty=2, col="red")
lines(x.grid, 1 - exp(-exp(LL95)), lty=2, col="red")
```

### **Generalized Additive Models (GAMs)**

#### **Terrorist Attacks Data**

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
YearlyAttacks <- read.csv("terrorism.dat", header = TRUE, sep = ";")
CountryData <- read.csv("countries.dat", header = TRUE, sep = ";")
YearlyAttacks <- merge(YearlyAttacks, CountryData, all = TRUE)
YearlyAttacks <- YearlyAttacks[order(YearlyAttacks$Year, YearlyAttacks$Country),] # Reorder the data
```

Look at the USA data:

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
wanted.rows <- grepl("United States",YearlyAttacks$Country, fixed = TRUE)
USAttacks <- YearlyAttacks[wanted.rows,]

par(lwd = 2, mar = c(3,3,2,2), mgp = c(2,0.75,0))
plot(USAttacks$Year, USAttacks$Freq, type = "l",col = "red",
     xlab = "Year",ylab = "Number of attacks",
     main = "Terrorist attacks in the USA, 1968-2009")
```

##### **Model 1 (Poisson GAM with log link)**

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
UStrends <- gam(Freq ~ s(Year), family = poisson(link = "log"),
                data = USAttacks)
summary(UStrends)
```

That's not very informative, except that it tells us that the smooth function of "Year" is highly significant (let's face it, this was obvious already!). Notice the use of `s(Year)` to represent [a "smooth function of Year"]{.underline}. The `edf` in the `summary()` output is "effective degrees of freedom", which is a [measure of the complexity of the fitted curve]{.underline}.

Check for overdispersion:

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
sum(resid(UStrends,type = "pearson")^2) / UStrends$df.residual
```

Not close to 1! Let's do a quasipoisson fit, therefore:

##### **Model 2 (quasipoisson GAM with log link)**

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
UStrends2 <- update(UStrends, family = quasipoisson(link = "log"))
summary(UStrends2)
```

In contrast to the GLM example for the Galapagos endemics, you'll notice here that the **coefficients** in the summary() output **do change a bit**. This is related to the way that the smoothing parameter is selected automatically. Almost all of the strange things about GAMs are related to this, in fact.

Plot the estimates of the smooth function:

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
plot(UStrends)
```

If we really want to show what the model tells us about trends in US terror attacks, we can do it using the `predict()` command, in exactly the same way that we did for the Galapagos endemics example. Like this:

```{r, eval=TRUE, echo=TRUE, warning=FALSE}
eta.predicted <- predict(UStrends2, se.fit = TRUE)
z <- qnorm(0.975, mean = 0, sd = 1, lower.tail = TRUE)
UL95 <- eta.predicted$fit + (z * eta.predicted$se.fit)
LL95 <- eta.predicted$fit - (z * eta.predicted$se.fit)
plot(USAttacks$Year, USAttacks$Freq, type = "p",col = "red",pch=15,
     xlab = "Year",ylab = "Number of attacks",
     main = "Terrorist attacks in the USA, 1968-2009")
lines(USAttacks$Year, exp(eta.predicted$fit), col = "darkred")
lines(USAttacks$Year, exp(UL95), col = "darkred", lty = 2)
lines(USAttacks$Year, exp(LL95), col = "darkred", lty = 2)
```
